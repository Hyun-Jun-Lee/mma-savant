"""
LangChain ê¸°ë°˜ LLM ì„œë¹„ìŠ¤ V2 - ë¦¬íŒ©í† ë§ëœ ë²„ì „
ëª¨ë“ˆí™”ëœ ì•„í‚¤í…ì²˜ë¡œ ë‹¤ì–‘í•œ LLM í”„ë¡œë°”ì´ë” ì§€ì› ë° í–¥ìƒëœ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
"""
import os
import asyncio
import uuid
import time
from typing import List, Dict, Any, Optional, AsyncGenerator, Tuple
from datetime import datetime
from traceback import format_exc

from langchain_core.messages import AIMessage, HumanMessage

from config import Config
from llm.model_factory import create_llm_with_callbacks, get_available_providers
from llm.agent_manager import AgentManager
from llm.exceptions import LLMException
from llm.stream_processor import (
    extract_safe_text_content,
    clean_response_content,
    create_final_result,
    create_error_response,
    validate_streaming_chunk
)
from common.utils import remove_timestamps_from_tool_result
from conversation.message_manager import ChatHistory
from database.connection.postgres_conn import get_async_db_context
from common.logging_config import get_logger
from common.utils import utc_now

LOGGER = get_logger(__name__)


class LangChainLLMService:
    """
    LangChain LLM ì„œë¹„ìŠ¤ V2 - ëª¨ë“ˆí™”ëœ ì•„í‚¤í…ì²˜
    """

    def __init__(self, max_cache_size: int = 5, provider: Optional[str] = None):
        """
        ì„œë¹„ìŠ¤ ì´ˆê¸°í™”

        Args:
            max_cache_size: íˆìŠ¤í† ë¦¬ ìºì‹œ í¬ê¸°
            provider: ì‚¬ìš©í•  LLM í”„ë¡œë°”ì´ë” (Noneì´ë©´ Configì—ì„œ ê²°ì •)
        """
        # ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ íŒ©í† ë¦¬ ì €ì¥ (ChatHistory ìƒì„±ì‹œ ì‚¬ìš©)
        self.async_db_session_factory = get_async_db_context
        self.max_cache_size = max_cache_size

        self.agent_manager = AgentManager()

        # LLM í”„ë¡œë°”ì´ë” ì„¤ì •
        self.provider = provider or Config.LLM_PROVIDER

    async def generate_streaming_chat_response(
        self,
        user_message: str,
        conversation_id: Optional[int] = None,
        user_id: Optional[int] = None,
        provider_override: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‘ë‹µ ìƒì„± ë©”ì¸ ì§„ì…ì 

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            conversation_id: ì„¸ì…˜ ID
            user_id: ì‚¬ìš©ì ID
            provider_override: í”„ë¡œë°”ì´ë” ì˜¤ë²„ë¼ì´ë“œ

        Yields:
            Dict[str, Any]: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬ë“¤
        """
        message_id = str(uuid.uuid4())
        start_time = time.time()

        # í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ ê²€ì¦
        validation_error = await self._validate_streaming_parameters(conversation_id, user_id, message_id)
        if validation_error:
            yield validation_error
            return

        try:
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ ë¡œë“œ
            history_result = await self._load_chat_history(conversation_id, user_id, message_id)
            if isinstance(history_result, dict):  # ì—ëŸ¬ ì‘ë‹µì¸ ê²½ìš°
                yield history_result
                return
            history = history_result

            # Two-Phase ì‹œìŠ¤í…œ ì„¤ì •
            llm, callback_handler, valid_chat_history = await self._setup_two_phase_system(
                provider_override, message_id, conversation_id, history
            )

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹¤í–‰
            async for chunk in self._execute_streaming_response(
                user_message, valid_chat_history, llm, callback_handler,
                history, message_id, conversation_id, user_id
            ):
                yield chunk

        except Exception as e:
            LOGGER.error(f"âŒ Main execution error: {e}")
            LOGGER.error(format_exc())
            yield {
                "type": "error",
                "error": str(e),
                "message_id": message_id,
                "conversation_id": conversation_id,
                "timestamp": utc_now().isoformat()
            }

        finally:
            # ì´ ì‹¤í–‰ ì‹œê°„ ë¡œê¹…
            total_time = time.time() - start_time
            LOGGER.info(f"â±ï¸ Total streaming function took: {total_time:.3f}s")

    async def _validate_streaming_parameters(
        self, conversation_id: Optional[int], user_id: Optional[int], message_id: str
    ) -> Optional[Dict[str, Any]]:
        """ìŠ¤íŠ¸ë¦¬ë° ë§¤ê°œë³€ìˆ˜ ê²€ì¦"""
        if not conversation_id:
            LOGGER.error("Session ID is required for streaming chat response")
            return create_error_response(
                ValueError("Session ID is required"),
                "unknown",
                "unknown"
            )

        if not user_id:
            LOGGER.error("User ID is required for streaming chat response")
            return create_error_response(
                ValueError("User ID is required"),
                "unknown",
                conversation_id
            )

        return None

    async def _load_chat_history(
        self, conversation_id : int, user_id: int, message_id: str
    ) -> Any:
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ ë¡œë“œ ë° ì—ëŸ¬ ì²˜ë¦¬"""
        try:
            history_start = time.time()
            # ë§¤ë²ˆ ìƒˆë¡œìš´ ChatHistory ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìƒˆ conversationì´ë¯€ë¡œ)
            history = ChatHistory(
                conversation_id=conversation_id,
                user_id=user_id,
                async_db_session_factory=self.async_db_session_factory,
                max_cache_size=self.max_cache_size
            )
            # ì´ˆê¸° ë¡œë“œ
            await history._ensure_loaded()
            history_time = time.time() - history_start
            LOGGER.info(f"â±ï¸ History loading: {history_time:.3f}s")
            LOGGER.info(f"ğŸ“š Loaded {len(history.messages)} messages from cache")
            return history

        except Exception as e:
            LOGGER.error(f"âŒ Error loading chat history: {e}")
            LOGGER.error(format_exc())
            return {
                "type": "error",
                "error": f"Failed to load chat history: {str(e)}",
                "message_id": message_id,
                "conversation_id": conversation_id,
                "timestamp": utc_now().isoformat()
            }

    async def _setup_two_phase_system(
        self, provider_override: Optional[str], message_id: str,
        conversation_id : int, history: Any
    ) -> Tuple[Any, Any, List]:
        """Two-Phase ì‹œìŠ¤í…œ ì„¤ì •"""
        # LLM ë° ì½œë°± ìƒì„±
        selected_provider = provider_override or self.provider
        llm, callback_handler = create_llm_with_callbacks(
            message_id=message_id,
            conversation_id=conversation_id,
            provider=selected_provider
        )
        LOGGER.info(f"ğŸ¤– Using provider for Two-Phase: {selected_provider}")

        # íˆìŠ¤í† ë¦¬ ê²€ì¦
        valid_chat_history = history.messages if hasattr(history, 'messages') else []
        LOGGER.info(f"ğŸ“š Using {len(valid_chat_history)} valid messages for Two-Phase context")
        LOGGER.info(f"ğŸ”§ Two-Phase system ready with ReAct agent and OpenRouter")

        return llm, callback_handler, valid_chat_history

    async def _execute_streaming_response(
        self, user_message: str, chat_history: List, llm: Any,
        callback_handler: Any, history: Any, message_id: str,
        conversation_id : int, user_id: int
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‹¤í–‰ ë° ê²€ì¦"""
        try:
            async for chunk in self._execute_two_phase_with_streaming(
                user_message=user_message,
                chat_history=chat_history,
                llm=llm,
                callback_handler=callback_handler,
                history=history,
                message_id=message_id,
                conversation_id=conversation_id,
                user_id=user_id
            ):
                # ì²­í¬ ìœ íš¨ì„± ê²€ì‚¬
                if validate_streaming_chunk(chunk):
                    yield chunk
                else:
                    LOGGER.warning(f"âš ï¸ Invalid streaming chunk filtered: {chunk}")

        except Exception as e:
            LOGGER.error(f"âŒ Error setting up Two-Phase system: {e}")
            LOGGER.error(format_exc())
            yield {
                "type": "error",
                "error": f"Failed to setup Two-Phase system: {str(e)}",
                "message_id": message_id,
                "conversation_id": conversation_id,
                "timestamp": utc_now().isoformat(),
                "two_phase_system": True
            }

    async def _execute_two_phase_with_streaming(
        self,
        user_message: str,
        chat_history: List,
        llm,
        callback_handler,
        history,
        message_id: str,
        conversation_id : int,
        user_id: Optional[int] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Two-Phase ì‹œìŠ¤í…œ ì‹¤í–‰ ë° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ë©”ì¸ ì§„ì…ì 
        """
        try:
            # Two-Phase ì‹¤í–‰ ì¤€ë¹„ ë° Phase start ì‹ í˜¸
            async for chunk in self._prepare_two_phase_execution(
                user_message, history, message_id, conversation_id
            ):
                yield chunk

            # Agent Two-Step ì‹¤í–‰
            result, execution_time = await self._execute_agent_two_step(
                user_message, chat_history, llm, callback_handler
            )

            # Agent ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬ ë° íˆìŠ¤í† ë¦¬ ì €ì¥
            async for chunk in self._process_agent_result(
                result, history, execution_time, message_id, conversation_id
            ):
                yield chunk

        except Exception as e:
            LOGGER.error(f"âŒ Two-Phase streaming error: {e}")
            error_response = self._handle_two_phase_error(e, message_id, conversation_id)
            yield error_response

    async def _prepare_two_phase_execution(
        self, user_message: str, history: Any, message_id: str, conversation_id : int
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Two-Phase ì‹¤í–‰ ì¤€ë¹„ ë° Phase start ì‹ í˜¸"""
        LOGGER.info("ğŸš€ Starting Two-Phase execution...")

        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        user_msg = HumanMessage(content=user_message)
        history.add_message(user_msg)

        # Phase 1: Understanding and Collection
        LOGGER.info("ğŸ“ Phase 1: Understanding and Collection")
        yield {
            "type": "phase_start",
            "phase": 1,
            "description": "Analyzing query and collecting data",
            "message_id": message_id,
            "conversation_id": conversation_id,
            "timestamp": utc_now().isoformat()
        }

    async def _execute_agent_two_step(
        self, user_message: str, chat_history: List, llm: Any, callback_handler: Any
    ) -> Tuple[Dict[str, Any], float]:
        """Agent Two-Step ì‹¤í–‰ ë° ì‹œê°„ ì¸¡ì •"""
        two_phase_start = time.time()

        result = await self.agent_manager.process_two_step(
            user_query=user_message,
            llm=llm,
            callback_handler=callback_handler,
            chat_history=chat_history
        )

        # ì—ëŸ¬ ì²´í¬ - agent_managerì—ì„œ ë°˜í™˜ëœ ì—ëŸ¬ ì‘ë‹µ ì²˜ë¦¬
        if result.get("error") is True:
            # ì´ë¯¸ êµ¬ì¡°í™”ëœ ì—ëŸ¬ ì‘ë‹µì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
            execution_time = time.time() - two_phase_start
            return result, execution_time

        execution_time = time.time() - two_phase_start
        LOGGER.info("âœ… Two-Phase execution completed")
        LOGGER.info(f"â±ï¸ Total Two-Phase execution took: {execution_time:.3f}s")

        return result, execution_time

    async def _process_agent_result(
        self, result: Dict[str, Any], history: Any, execution_time: float,
        message_id: str, conversation_id : int
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """Agent ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬ ë° íˆìŠ¤í† ë¦¬ ì €ì¥"""
        # ì—ëŸ¬ ì‘ë‹µì¸ ê²½ìš° ë°”ë¡œ ë°˜í™˜
        if result.get("error") is True:
            yield {
                **result,  # error, error_class, traceback í¬í•¨
                "type": "error_response",  # í”„ë¡ íŠ¸ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” íƒ€ì…
                "message_id": message_id,
                "conversation_id": conversation_id,
                "timestamp": utc_now().isoformat(),
                "total_execution_time": execution_time
            }
            return

        # AI ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ì‹œê°í™” ì •ë³´ëŠ” ì €ì¥í•˜ì§€ ì•Šê³  ê°„ë‹¨í•œ ìš”ì•½ë§Œ)
        summary_content = f"MMA ë°ì´í„° ë¶„ì„ ì™„ë£Œ: {result.get('visualization_type', 'unknown')} ì°¨íŠ¸, {result.get('row_count', 0)}ê°œ ë°ì´í„°"
        ai_message = AIMessage(
            content=summary_content,
            additional_kwargs={
                "two_phase_system": True,
                "visualization_type": result.get('visualization_type'),
                "row_count": result.get('row_count', 0)
            }
        )
        history.add_message(ai_message)

        # ìµœì¢… ê²°ê³¼ ë°˜í™˜
        yield {
            **result,  # process_two_stepì˜ ê²°ê³¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            "type": "final_result",
            "message_id": message_id,
            "conversation_id": conversation_id,
            "timestamp": utc_now().isoformat(),
            "total_execution_time": execution_time
        }

    def _handle_two_phase_error(
        self, error: Exception, message_id: str, conversation_id : int
    ) -> Dict[str, Any]:
        """Two-Phase ì—ëŸ¬ ì²˜ë¦¬ (LLMException êµ¬ì¡°í™” í¬í•¨)"""
        LOGGER.error(f"âŒ Two-Phase execution failed: {error}")
        LOGGER.error(format_exc())

        # LLMExceptionì¸ ê²½ìš° êµ¬ì¡°í™”ëœ ì—ëŸ¬ ì‘ë‹µ ë°˜í™˜
        if isinstance(error, LLMException):
            return {
                "type": "error_response",
                "error": True,
                "error_class": error.error_class,
                "traceback": format_exc(),
                "message_id": message_id,
                "conversation_id": conversation_id,
                "timestamp": utc_now().isoformat(),
                "two_phase_system": True
            }

        # ì¼ë°˜ Exception ì²˜ë¦¬ (Rate limit ë“±)
        error_message = str(error)
        if "rate_limit_error" in error_message or "429" in error_message:
            LOGGER.warning("ğŸš« Rate limit exceeded - reducing token usage recommended")
            error_message = "API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

        return {
            "type": "error",
            "error": error_message,
            "message_id": message_id,
            "conversation_id": conversation_id,
            "timestamp": utc_now().isoformat(),
            "two_phase_system": True
        }

    def get_conversation_starter(self) -> str:
        """ëŒ€í™” ì‹œì‘ ë©”ì‹œì§€ ë°˜í™˜"""
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë”ì— ë”°ë¥¸ ë§ì¶¤ ë©”ì‹œì§€
            available_providers = get_available_providers()
            provider_info = f" (Using {self.provider})" if len(available_providers) > 1 else ""

            return f"ì•ˆë…•í•˜ì„¸ìš”! MMA Savantì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤{provider_info}. MMAì— ê´€í•œ ëª¨ë“  ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”!"

        except Exception as e:
            LOGGER.error(f"âŒ Error getting conversation starter: {e}")
            return "ì•ˆë…•í•˜ì„¸ìš”! MMAì— ê´€í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."

    async def health_check(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            # ê¸°ë³¸ ìƒíƒœ
            health_status = {
                "service": "LangChainLLMServiceV2",
                "status": "healthy",
                "timestamp": utc_now().isoformat(),
                "version": "2.0"
            }

            # í”„ë¡œë°”ì´ë” ìƒíƒœ
            available_providers = get_available_providers()
            health_status.update({
                "llm_provider": self.provider,
                "available_providers": available_providers,
                "providers_count": len(available_providers)
            })

            # Two-Phase ì‹œìŠ¤í…œ ìƒíƒœ
            agent_health = await self.agent_manager.health_check()
            health_status["two_phase_system"] = agent_health
            health_status["agent_manager_version"] = "v2"

            return health_status

        except Exception as e:
            LOGGER.error(f"âŒ Health check error: {e}")
            return {
                "service": "LangChainLLMServiceV2",
                "status": "error",
                "error": str(e),
                "timestamp": utc_now().isoformat(),
                "two_phase_system": True
            }

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # Two-Phase ì‹œìŠ¤í…œ ì •ë¦¬ (AgentManagerëŠ” MCP ìºì‹œê°€ ì—†ìŒ)
            if hasattr(self, 'agent_manager'):
                LOGGER.info("ğŸ§¹ AgentManager cleanup (no MCP cache to clear)")

            LOGGER.info("âœ… Two-Phase LLM service V2 cleanup completed")

        except Exception as e:
            LOGGER.error(f"âŒ Two-Phase cleanup error: {e}")


# ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
_langchain_service_v2 = None


async def get_langchain_service(
    provider: Optional[str] = None,
    max_cache_size: int = 5
) -> LangChainLLMService:
    """
    ê¸€ë¡œë²Œ LangChain ì„œë¹„ìŠ¤ V2 ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    Args:
        provider: LLM í”„ë¡œë°”ì´ë” (ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œì—ë§Œ ì ìš©)
        max_cache_size: ìºì‹œ í¬ê¸° (ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œì—ë§Œ ì ìš©)

    Returns:
        LangChainLLMService: ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
    """
    global _langchain_service_v2

    if _langchain_service_v2 is None:
        _langchain_service_v2 = LangChainLLMService(
            max_cache_size=max_cache_size,
            provider=provider
        )
        LOGGER.info("âœ… Two-Phase LangChain service V2 created")

    return _langchain_service_v2


# í¸ì˜ í•¨ìˆ˜ë“¤
async def create_streaming_response(
    user_message: str,
    conversation_id : int,
    user_id: int,
    provider: Optional[str] = None
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± í¸ì˜ í•¨ìˆ˜

    Args:
        user_message: ì‚¬ìš©ì ë©”ì‹œì§€
        conversation_id: ì„¸ì…˜ ID
        user_id: ì‚¬ìš©ì ID
        provider: í”„ë¡œë°”ì´ë” ì˜¤ë²„ë¼ì´ë“œ

    Yields:
        Dict[str, Any]: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬ë“¤
    """
    service = await get_langchain_service(provider=provider)

    async for chunk in service.generate_streaming_chat_response(
        user_message=user_message,
        conversation_id=conversation_id,
        user_id=user_id,
        provider_override=provider
    ):
        yield chunk


if __name__ == "__main__":
    """í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹…ìš©"""
    import asyncio

    async def test_service_v2():
        print("ğŸš€ Two-Phase LangChain Service V2 Test")
        print("=" * 50)

        try:
            # ì„œë¹„ìŠ¤ ìƒì„±
            service = await get_langchain_service()

            # ìƒíƒœ í™•ì¸
            health = await service.health_check()
            print(f"\nğŸ¥ Health Check:")
            print(f"  Status: {health['status']}")
            print(f"  Provider: {health['llm_provider']}")
            print(f"  Available Providers: {health['available_providers']}")

            # ëŒ€í™” ì‹œì‘ ë©”ì‹œì§€
            starter = service.get_conversation_starter()
            print(f"\nğŸ’¬ Conversation Starter:")
            print(f"  {starter}")

            print(f"\nâœ… Two-Phase Service V2 test completed successfully")

        except Exception as e:
            print(f"\nâŒ Two-Phase Service V2 test failed: {e}")
            import traceback
            traceback.print_exc()

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    if asyncio.get_event_loop().is_running():
        print("Running in existing event loop - skipping test")
    else:
        asyncio.run(test_service_v2())