"""
LangChain ê¸°ë°˜ LLM ì„œë¹„ìŠ¤
ë‹¨ì¼ MCP ì„œë²„ìš© ìµœì í™”ëœ êµ¬í˜„
"""
import os
import asyncio
import uuid
import time
import json
from typing import List, Dict, Any, Optional, AsyncGenerator
from datetime import datetime
from traceback import format_exc
from contextlib import asynccontextmanager

from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import AIMessage, SystemMessage
from langchain_core.tools import BaseTool
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.callbacks.base import AsyncCallbackHandler
from langchain.schema import LLMResult
from langchain_mcp_adapters.tools import load_mcp_tools
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from config import Config
from llm.prompts.en_ver import get_en_system_prompt_with_tools
from conversation.message_manager import ChatHistoryManager
from database.connection.postgres_conn import get_async_db_context
from common.logging_config import get_logger

LOGGER = get_logger(__name__)


class StreamingCallbackHandler(AsyncCallbackHandler):
    """ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬"""
    
    def __init__(self, message_id: str, session_id: str):
        self.tokens = []
        self.message_id = message_id
        self.session_id = session_id
        self.current_content = ""
        self.stream_queue = asyncio.Queue()
        self.is_streaming = False
        self.tool_calls = []
    
    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        """ìƒˆ í† í°ì´ ìƒì„±ë  ë•Œ í˜¸ì¶œ - ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë°"""
        try:
            token_str = ""
            
            # Anthropicì˜ í† í° í˜•ì‹ ì²˜ë¦¬ ë° íˆ´ í˜¸ì¶œ í† í° í•„í„°ë§
            if isinstance(token, dict):
                # íˆ´ í˜¸ì¶œ ê´€ë ¨ í† í°ë“¤ í•„í„°ë§
                tool_types = ['tool_use', 'input_json_delta', 'tool_call', 'function_call']
                if token.get('type') in tool_types:
                    return  # íˆ´ ê´€ë ¨ í† í°ì€ ìŠ¤íŠ¸ë¦¬ë°í•˜ì§€ ì•ŠìŒ
                
                # íˆ´ í˜¸ì¶œ IDë‚˜ ì´ë¦„ì´ í¬í•¨ëœ í† í° í•„í„°ë§
                if 'id' in token and token.get('id', '').startswith('toolu_'):
                    return
                
                # {'text': 'content', 'type': 'text', 'index': 0} í˜•ì‹
                if 'text' in token:
                    token_str = token['text']
                else:
                    return  # textê°€ ì—†ëŠ” í† í°ì€ ìŠ¤íŠ¸ë¦¬ë°í•˜ì§€ ì•ŠìŒ
                    
            elif isinstance(token, list):
                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê° ìš”ì†Œì—ì„œ text ì¶”ì¶œ
                texts = []
                for item in token:
                    if isinstance(item, dict):
                        # íˆ´ ê´€ë ¨ í† í° í•„í„°ë§
                        tool_types = ['tool_use', 'input_json_delta', 'tool_call', 'function_call']
                        if item.get('type') in tool_types:
                            continue
                        # íˆ´ ID í† í° í•„í„°ë§
                        if 'id' in item and item.get('id', '').startswith('toolu_'):
                            continue
                        if 'text' in item:
                            texts.append(item['text'])
                    else:
                        texts.append(str(item))
                token_str = ''.join(texts)
                if not token_str:
                    return  # ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ìŠ¤íŠ¸ë¦¬ë°í•˜ì§€ ì•ŠìŒ
            else:
                token_str = str(token)
            
            if token_str:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì²˜ë¦¬
                self.tokens.append(token_str)
                self.current_content += token_str
                
                # ìŠ¤íŠ¸ë¦¬ë° íì— í† í° ì¶”ê°€
                await self.stream_queue.put({
                    "type": "content",
                    "content": token_str,
                    "message_id": self.message_id,
                    "session_id": self.session_id,
                    "timestamp": datetime.now().isoformat()
                })
                
        except Exception as e:
            print(f"âŒ Error in on_llm_new_token: {e}")
            print(f"ğŸ” Token type: {type(token)}, value: {token}")
    
    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs) -> None:
        """LLM ì‹œì‘ ì‹œ í˜¸ì¶œ"""
        self.tokens = []
        self.current_content = ""
        self.is_streaming = True
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì‹ í˜¸
        await self.stream_queue.put({
            "type": "start",
            "message_id": self.message_id,
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat()
        })
    
    async def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        """LLM ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        self.is_streaming = False
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì‹ í˜¸
        await self.stream_queue.put({
            "type": "end",
            "message_id": self.message_id,
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat(),
            "final_content": self.current_content
        })
    
    async def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs) -> None:
        """íˆ´ ì‹œì‘ ì‹œ í˜¸ì¶œ"""
        tool_name = serialized.get("name", "unknown")
        tool_start_time = time.time()
        self.tool_calls.append({
            "tool": tool_name,
            "input": input_str,
            "status": "started",
            "start_time": tool_start_time
        })
        
        print(f"ğŸ”§ Tool '{tool_name}' started at {tool_start_time}")
        
        # íˆ´ ì‚¬ìš© ì•Œë¦¼
        await self.stream_queue.put({
            "type": "tool_start",
            "tool_name": tool_name,
            "tool_input": input_str,
            "message_id": self.message_id,
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat()
        })
    
    async def on_tool_end(self, output: str, **kwargs) -> None:
        """íˆ´ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        tool_end_time = time.time()
        
        if self.tool_calls:
            tool_call = self.tool_calls[-1]
            tool_call["status"] = "completed"
            tool_call["result"] = output[:200] + "..." if len(output) > 200 else output
            tool_call["end_time"] = tool_end_time
            
            # íˆ´ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            if "start_time" in tool_call:
                tool_duration = tool_end_time - tool_call["start_time"]
                tool_call["duration"] = tool_duration
                print(f"ğŸ”§ Tool '{tool_call['tool']}' completed in {tool_duration:.3f}s")
            else:
                print(f"ğŸ”§ Tool completed at {tool_end_time}")
        
        # íˆ´ ì™„ë£Œ ì•Œë¦¼
        await self.stream_queue.put({
            "type": "tool_end",
            "tool_result": output[:200] + "..." if len(output) > 200 else output,
            "message_id": self.message_id,
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat()
        })
    
    async def on_agent_action(self, action, **kwargs) -> None:
        """ì—ì´ì „íŠ¸ ì•¡ì…˜ ì‹œ í˜¸ì¶œ"""
        await self.stream_queue.put({
            "type": "thinking",
            "thought": f"Using tool: {action.tool}",
            "message_id": self.message_id,
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat()
        })


class LangChainLLMService:
    """LangChain ê¸°ë°˜ LLM ì„œë¹„ìŠ¤ - ë‹¨ì¼ MCP ì„œë²„ ìµœì í™”"""
    
    def __init__(self, max_cache_size: int = 100):
        # Anthropic LLM ì´ˆê¸°í™”
        self.llm = ChatAnthropic(
            api_key=Config.ANTHROPIC_API_KEY,
            model=Config.ANTHROPIC_MODEL_NAME,
            temperature=0.7,
            max_tokens=4000,
            streaming=True
        )

        self.history_manager = ChatHistoryManager(
            async_db_session_factory=get_async_db_context,
            max_cache_size=max_cache_size
        )
        
        # ë‹¨ì¼ MCP ì„œë²„ ì„¤ì •
        current_dir = os.path.dirname(os.path.abspath(__file__))
        src_dir = os.path.dirname(current_dir)
        mcp_server_path = os.path.join(src_dir, "tools", "mcp_server.py")
        
        self.server_params = StdioServerParameters(
            command="python",
            args=[mcp_server_path],
        )
        
        # MCP ë„êµ¬ ìºì‹±
        self._cached_tools = None
        self._tools_loading = False
    
    @asynccontextmanager
    async def _get_mcp_tools(self):
        """
        MCP ë„êµ¬ë“¤ì„ context managerë¡œ ì œê³µ (ì„¸ì…˜ ìœ ì§€)
        """
        print("ğŸ”„ Loading MCP tools...")
        async with stdio_client(self.server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # ì—°ê²° ì´ˆê¸°í™”
                await session.initialize()
                
                # ë„êµ¬ ë¡œë“œ
                tools = await load_mcp_tools(session)
                print(f"âœ… MCP Tools loaded: {len(tools)} tools")
                
                yield tools

    async def generate_streaming_chat_response(
        self,
        user_message: str,
        conversation_history: List[Dict[str, str]] = None,
        session_id: Optional[str] = None,
        user_id: Optional[int] = None,
        ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‘ë‹µ ìƒì„±
        """
        if not session_id:
            LOGGER.error("Session ID is required for streaming chat response")
            raise ValueError("Session ID is required for streaming chat response")

        if not user_id:
            LOGGER.error("User ID is required for streaming chat response")
            raise ValueError("User ID is required for streaming chat response")

        message_id = str(uuid.uuid4())
        start_time = time.time()
        
        try:
            # Chat history ê°€ì ¸ì˜¤ê¸°
            history_start = time.time()
            history = await self.history_manager.get_session_history(session_id, user_id)
            history_time = time.time() - history_start
            LOGGER.info(f"â±ï¸ History loading: {history_time:.3f}s")
            LOGGER.info(f"ğŸ“š Loaded {len(history.messages)} messages from cache")

            # ì‚¬ìš©ì ë©”ì‹œì§€ëŠ” RunnableWithMessageHistoryê°€ ìë™ìœ¼ë¡œ ì¶”ê°€í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ

            # ë‹¨ì¼ MCP ì„œë²„ì—ì„œ ë„êµ¬ ë¡œë“œ
            mcp_start = time.time()
            async with self._get_mcp_tools() as tools:
                mcp_time = time.time() - mcp_start
                print(f"â±ï¸ MCP tools loading took: {mcp_time:.3f}s")
                
                # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
                callback_handler = StreamingCallbackHandler(message_id, session_id)
                
                # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
                prompt = ChatPromptTemplate.from_messages([
                    ("system", get_en_system_prompt_with_tools()),
                    ("human", "{input}"),
                    ("placeholder", "{agent_scratchpad}"),
                ])
                
                # ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ëŠ” LLM ìƒì„±
                streaming_llm = ChatAnthropic(
                    api_key=Config.ANTHROPIC_API_KEY,
                    model=Config.ANTHROPIC_MODEL_NAME,
                    temperature=0.7,
                    max_tokens=4000,
                    streaming=True,
                    callbacks=[callback_handler]  
                )
                
                # ì—ì´ì „íŠ¸ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° LLM ì‚¬ìš©)
                agent = create_tool_calling_agent(streaming_llm, tools, prompt)

                agent_executor = AgentExecutor(
                    agent=agent, 
                    tools=tools, 
                    verbose=True,
                    return_intermediate_steps=True,  
                    callbacks=[callback_handler]  
                )

                # íˆìŠ¤í† ë¦¬ì™€ í•¨ê»˜ ì‹¤í–‰í•˜ëŠ” ì²´ì¸ ìƒì„±
                def get_session_history_sync(sid: str):
                    return history  # ì´ë¯¸ ë¡œë“œëœ íˆìŠ¤í† ë¦¬ ë°˜í™˜
                
                chain_with_history = RunnableWithMessageHistory(
                    agent_executor,
                    get_session_history_sync,
                    input_messages_key="input",
                    history_messages_key="chat_history"
                )
                
                print(f"ğŸ¤– Agent created with {len(tools)} tools")
                
                async def run_agent():
                    try:
                        LOGGER.info("ğŸš€ Starting agent execution...")
                        
                        agent_exec_start = time.time()
                        invoke_start = time.time()
                        
                        # RunnableWithMessageHistoryë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰
                        config = {"configurable": {"session_id": session_id}}
                        response_content = ""
                        tool_results = []
                        
                        # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê²°ê³¼ ì²˜ë¦¬
                        async for chunk in chain_with_history.astream(
                            {"input": user_message},
                            config=config
                        ):
                            if isinstance(chunk, dict):
                                if "output" in chunk:
                                    content = chunk["output"]
                                    # íƒ€ì… ì•ˆì „ì„± í™•ë³´
                                    if isinstance(content, str):
                                        response_content += content
                                    elif isinstance(content, list):
                                        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
                                        response_content += " ".join(str(item) for item in content)
                                    else:
                                        # ê¸°íƒ€ íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
                                        response_content += str(content)
                                
                                if "intermediate_steps" in chunk:
                                    steps = chunk["intermediate_steps"]
                                    for step in steps:
                                        if len(step) >= 2:
                                            action, observation = step
                                            tool_results.append({
                                                "tool": getattr(action, 'tool', 'unknown'),
                                                "input": str(action.tool_input),
                                                "result": str(observation)[:500]
                                            })
                        
                        invoke_time = time.time() - invoke_start
                        agent_exec_time = time.time() - agent_exec_start
                        
                        LOGGER.info("âœ… Agent execution completed")
                        LOGGER.info(f"â±ï¸ Chain execution took: {invoke_time:.3f}s")
                        LOGGER.info(f"â±ï¸ Total agent execution took: {agent_exec_time:.3f}s")
                        
                        # AI ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ë©”ëª¨ë¦¬ ì¦‰ì‹œ + DB ë°±ê·¸ë¼ìš´ë“œ)
                        if response_content:
                            ai_message = AIMessage(
                                content=response_content,
                                additional_kwargs={"tool_results": tool_results} if tool_results else {}
                            )
                            history.add_message(ai_message)
                        
                        # ê²°ê³¼ ë¶„ì„ ë¡œê¹…
                        if tool_results:
                            LOGGER.info(f"ğŸ“Š Agent used {len(tool_results)} tools")
                            for i, tool_result in enumerate(tool_results):
                                LOGGER.info(f"   Step {i+1}: Used tool '{tool_result['tool']}'")
                        
                        # ìµœì¢… ê²°ê³¼ íì— ì¶”ê°€
                        await callback_handler.stream_queue.put({
                            "type": "final_result",
                            "content": response_content,
                            "tool_results": tool_results,
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        })
                        
                    except Exception as e:
                        LOGGER.error(f"âŒ Agent execution failed: {e}")
                        LOGGER.error(format_exc())
                        
                        # Rate limit ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬
                        error_message = str(e)
                        if "rate_limit_error" in error_message or "429" in error_message:
                            LOGGER.warning("ğŸš« Rate limit exceeded - reducing token usage recommended")
                            error_message = "API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        
                        await callback_handler.stream_queue.put({
                            "type": "error",
                            "error": error_message,
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        })
                    finally:
                        # ì¢…ë£Œ ì‹ í˜¸
                        await callback_handler.stream_queue.put(None)
                
                # ì—ì´ì „íŠ¸ ì‹¤í–‰ íƒœìŠ¤í¬ ì‹œì‘
                agent_task = asyncio.create_task(run_agent())
                
                # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ yield
                first_chunk_time = None
                chunk_count = 0
                while True:
                    try:
                        # íƒ€ì„ì•„ì›ƒì„ ë‘ì–´ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
                        chunk = await asyncio.wait_for(
                            callback_handler.stream_queue.get(), 
                            timeout=60.0
                        )
                        
                        if chunk is None:  # ì¢…ë£Œ ì‹ í˜¸
                            break
                        
                        # ì²« ë²ˆì§¸ ì‘ë‹µ ì²­í¬ ì‹œê°„ ì¸¡ì •
                        if first_chunk_time is None and chunk.get("type") == "content":
                            first_chunk_time = time.time() - start_time
                            print(f"â±ï¸ First response chunk took: {first_chunk_time:.3f}s")
                        
                        chunk_count += 1
                        
                        yield chunk
                        
                    except asyncio.TimeoutError:
                        LOGGER.warning("âš ï¸ Streaming timeout - sending error")
                        yield {
                            "type": "error",
                            "error": "Streaming timeout",
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        }
                        break
                    except Exception as e:
                        LOGGER.error(f"âŒ Error in streaming: {e}")
                        LOGGER.error(format_exc())
                        yield {
                            "type": "error",
                            "error": str(e),
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        }
                        break
                
                try:
                    await agent_task
                except Exception as e:
                    LOGGER.error(f"âŒ Agent task error: {e}")
                    LOGGER.error(format_exc())
                
                total_time = time.time() - start_time
                LOGGER.info(f"â±ï¸ Total streaming function took: {total_time:.3f}s")
                
        except Exception as e:
            LOGGER.error(f"âŒ Error in streaming: {e}")
            LOGGER.error(format_exc())
            yield {
                "type": "error",
                "error": str(e),
                "message_id": message_id,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }

    def get_conversation_starter(self) -> str:
        """ëŒ€í™” ì‹œì‘ ë©”ì‹œì§€ ë°˜í™˜"""
        return get_conversation_starter()
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - MCP context managerê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬"""
        LOGGER.info("âœ… LLM service cleanup completed")


# ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ - ê°„ë‹¨í•œ ì‹±ê¸€í†¤
_langchain_service = None

async def get_langchain_service() -> LangChainLLMService:
    """ê¸€ë¡œë²Œ LangChain ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _langchain_service
    
    if _langchain_service is None:
        _langchain_service = LangChainLLMService()
        LOGGER.info("âœ… Single MCP server LangChain service created")
    
    return _langchain_service