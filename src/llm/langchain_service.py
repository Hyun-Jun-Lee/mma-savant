"""
LangChain ê¸°ë°˜ LLM ì„œë¹„ìŠ¤
ë‹¨ì¼ MCP ì„œë²„ìš© ìµœì í™”ëœ êµ¬í˜„
"""
import asyncio
import uuid
import time
import json
from typing import List, Dict, Any, Optional, AsyncGenerator
from datetime import datetime
from traceback import format_exc
from contextlib import asynccontextmanager

from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import BaseTool
from langchain.callbacks.base import AsyncCallbackHandler
from langchain.schema import LLMResult

# ë‹¨ì¼ MCP ì„œë²„ìš© imports
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from langchain_mcp_adapters.tools import load_mcp_tools

from config import Config
from llm.prompts.en_ver import get_en_system_prompt_with_tools, get_en_conversation_starter


class StreamingCallbackHandler(AsyncCallbackHandler):
    """ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬"""
    
    def __init__(self, message_id: str, session_id: str):
        self.tokens = []
        self.message_id = message_id
        self.session_id = session_id
        self.current_content = ""
        self.stream_queue = asyncio.Queue()
        self.is_streaming = False
        self.tool_calls = []
    
    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        """ìƒˆ í† í°ì´ ìƒì„±ë  ë•Œ í˜¸ì¶œ - ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë°"""
        try:
            token_str = ""
            
            # Anthropicì˜ í† í° í˜•ì‹ ì²˜ë¦¬ ë° íˆ´ í˜¸ì¶œ í† í° í•„í„°ë§
            if isinstance(token, dict):
                # íˆ´ í˜¸ì¶œ ê´€ë ¨ í† í°ë“¤ í•„í„°ë§
                tool_types = ['tool_use', 'input_json_delta', 'tool_call', 'function_call']
                if token.get('type') in tool_types:
                    return  # íˆ´ ê´€ë ¨ í† í°ì€ ìŠ¤íŠ¸ë¦¬ë°í•˜ì§€ ì•ŠìŒ
                
                # íˆ´ í˜¸ì¶œ IDë‚˜ ì´ë¦„ì´ í¬í•¨ëœ í† í° í•„í„°ë§
                if 'id' in token and token.get('id', '').startswith('toolu_'):
                    return
                
                # {'text': 'content', 'type': 'text', 'index': 0} í˜•ì‹
                if 'text' in token:
                    token_str = token['text']
                else:
                    return  # textê°€ ì—†ëŠ” í† í°ì€ ìŠ¤íŠ¸ë¦¬ë°í•˜ì§€ ì•ŠìŒ
                    
            elif isinstance(token, list):
                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê° ìš”ì†Œì—ì„œ text ì¶”ì¶œ
                texts = []
                for item in token:
                    if isinstance(item, dict):
                        # íˆ´ ê´€ë ¨ í† í° í•„í„°ë§
                        tool_types = ['tool_use', 'input_json_delta', 'tool_call', 'function_call']
                        if item.get('type') in tool_types:
                            continue
                        # íˆ´ ID í† í° í•„í„°ë§
                        if 'id' in item and item.get('id', '').startswith('toolu_'):
                            continue
                        if 'text' in item:
                            texts.append(item['text'])
                    else:
                        texts.append(str(item))
                token_str = ''.join(texts)
                if not token_str:
                    return  # ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ìŠ¤íŠ¸ë¦¬ë°í•˜ì§€ ì•ŠìŒ
            else:
                token_str = str(token)
            
            if token_str:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì²˜ë¦¬
                self.tokens.append(token_str)
                self.current_content += token_str
                
                # ìŠ¤íŠ¸ë¦¬ë° íì— í† í° ì¶”ê°€
                await self.stream_queue.put({
                    "type": "content",
                    "content": token_str,
                    "message_id": self.message_id,
                    "session_id": self.session_id,
                    "timestamp": datetime.now().isoformat()
                })
                
        except Exception as e:
            print(f"âŒ Error in on_llm_new_token: {e}")
            print(f"ğŸ” Token type: {type(token)}, value: {token}")
    
    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs) -> None:
        """LLM ì‹œì‘ ì‹œ í˜¸ì¶œ"""
        self.tokens = []
        self.current_content = ""
        self.is_streaming = True
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ ì‹ í˜¸
        await self.stream_queue.put({
            "type": "start",
            "message_id": self.message_id,
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat()
        })
    
    async def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        """LLM ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        self.is_streaming = False
        
        # ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì‹ í˜¸
        await self.stream_queue.put({
            "type": "end",
            "message_id": self.message_id,
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat(),
            "final_content": self.current_content
        })
    
    async def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs) -> None:
        """íˆ´ ì‹œì‘ ì‹œ í˜¸ì¶œ"""
        tool_name = serialized.get("name", "unknown")
        tool_start_time = time.time()
        self.tool_calls.append({
            "tool": tool_name,
            "input": input_str,
            "status": "started",
            "start_time": tool_start_time
        })
        
        print(f"ğŸ”§ Tool '{tool_name}' started at {tool_start_time}")
        
        # íˆ´ ì‚¬ìš© ì•Œë¦¼
        await self.stream_queue.put({
            "type": "tool_start",
            "tool_name": tool_name,
            "tool_input": input_str,
            "message_id": self.message_id,
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat()
        })
    
    async def on_tool_end(self, output: str, **kwargs) -> None:
        """íˆ´ ì¢…ë£Œ ì‹œ í˜¸ì¶œ"""
        tool_end_time = time.time()
        
        if self.tool_calls:
            tool_call = self.tool_calls[-1]
            tool_call["status"] = "completed"
            tool_call["result"] = output[:200] + "..." if len(output) > 200 else output
            tool_call["end_time"] = tool_end_time
            
            # íˆ´ ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
            if "start_time" in tool_call:
                tool_duration = tool_end_time - tool_call["start_time"]
                tool_call["duration"] = tool_duration
                print(f"ğŸ”§ Tool '{tool_call['tool']}' completed in {tool_duration:.3f}s")
            else:
                print(f"ğŸ”§ Tool completed at {tool_end_time}")
        
        # íˆ´ ì™„ë£Œ ì•Œë¦¼
        await self.stream_queue.put({
            "type": "tool_end",
            "tool_result": output[:200] + "..." if len(output) > 200 else output,
            "message_id": self.message_id,
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat()
        })
    
    async def on_agent_action(self, action, **kwargs) -> None:
        """ì—ì´ì „íŠ¸ ì•¡ì…˜ ì‹œ í˜¸ì¶œ"""
        await self.stream_queue.put({
            "type": "thinking",
            "thought": f"Using tool: {action.tool}",
            "message_id": self.message_id,
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat()
        })


class LangChainLLMService:
    """LangChain ê¸°ë°˜ LLM ì„œë¹„ìŠ¤ - ë‹¨ì¼ MCP ì„œë²„ ìµœì í™”"""
    
    def __init__(self):
        # Anthropic LLM ì´ˆê¸°í™”
        self.llm = ChatAnthropic(
            api_key=Config.ANTHROPIC_API_KEY,
            model=Config.ANTHROPIC_MODEL_NAME,
            temperature=0.7,
            max_tokens=4000,
            streaming=True
        )
        
        # ë‹¨ì¼ MCP ì„œë²„ ì„¤ì •
        import os
        current_dir = os.path.dirname(os.path.abspath(__file__))
        src_dir = os.path.dirname(current_dir)
        mcp_server_path = os.path.join(src_dir, "tools", "mcp_server.py")
        
        self.server_params = StdioServerParameters(
            command="python",
            args=[mcp_server_path],
        )
    
    @asynccontextmanager
    async def _get_mcp_tools(self):
        """
        ë‹¨ì¼ MCP ì„œë²„ì—ì„œ ë„êµ¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ìµœì í™”ëœ context manager
        """
        async with stdio_client(self.server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # ì—°ê²° ì´ˆê¸°í™”
                await session.initialize()
                
                # ë„êµ¬ ë¡œë“œ
                tools = await load_mcp_tools(session)
                print(f"âœ… MCP Tools loaded: {len(tools)} tools")
                
                yield tools

    async def generate_streaming_chat_response(
        self,
        user_message: str,
        conversation_history: List[Dict[str, str]] = None,
        session_id: Optional[str] = None
        ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‘ë‹µ ìƒì„±
        """
        message_id = str(uuid.uuid4())
        start_time = time.time()
        
        try:
            # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì¤€ë¹„
            prep_start = time.time()
            messages = self._prepare_messages(user_message, conversation_history)
            prep_time = time.time() - prep_start

            print("-"*50)
            print("check message :")
            print(messages)
            print("-"*50)
            
            # ë‹¨ì¼ MCP ì„œë²„ì—ì„œ ë„êµ¬ ë¡œë“œ
            mcp_start = time.time()
            async with self._get_mcp_tools() as tools:
                mcp_time = time.time() - mcp_start
                print(f"â±ï¸ MCP tools loading took: {mcp_time:.3f}s")
                
                # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
                callback_handler = StreamingCallbackHandler(message_id, session_id)
                
                # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
                prompt = ChatPromptTemplate.from_messages([
                    ("system", get_en_system_prompt_with_tools()),
                    ("human", "{input}"),
                    ("placeholder", "{agent_scratchpad}"),
                ])
                
                # ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•˜ëŠ” LLM ìƒì„±
                streaming_llm = ChatAnthropic(
                    api_key=Config.ANTHROPIC_API_KEY,
                    model=Config.ANTHROPIC_MODEL_NAME,
                    temperature=0.7,
                    max_tokens=4000,
                    streaming=True,
                    callbacks=[callback_handler]  
                )
                
                # ì—ì´ì „íŠ¸ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° LLM ì‚¬ìš©)
                agent = create_tool_calling_agent(streaming_llm, tools, prompt)

                agent_executor = AgentExecutor(
                    agent=agent, 
                    tools=tools, 
                    verbose=True,
                    return_intermediate_steps=True,  
                    callbacks=[callback_handler]  
                )
                
                print(f"ğŸ¤– Agent created with {len(tools)} tools")
                
                # ë¹„ë™ê¸° ì—ì´ì „íŠ¸ ì‹¤í–‰ì„ ë³„ë„ íƒœìŠ¤í¬ë¡œ ì‹œì‘
                async def run_agent():
                    try:
                        print("ğŸš€ Starting agent execution...")
                        
                        agent_exec_start = time.time()
                        
                        invoke_start = time.time()
                        
                        result = await agent_executor.ainvoke({
                            "input": user_message,
                            "chat_history": messages[:-1]
                        })
                        
                        invoke_time = time.time() - invoke_start
                        agent_exec_time = time.time() - agent_exec_start
                        
                        print("âœ… Agent execution completed")
                        print(f"â±ï¸ ainvoke() call took: {invoke_time:.3f}s")
                        print(f"â±ï¸ Total agent execution took: {agent_exec_time:.3f}s")
                        
                        # ê²°ê³¼ ë¶„ì„
                        if "intermediate_steps" in result:
                            steps = result["intermediate_steps"]
                            print(f"ğŸ“Š Agent used {len(steps)} intermediate steps")
                            for i, step in enumerate(steps):
                                if hasattr(step, '__len__') and len(step) >= 2:
                                    action, observation = step
                                    tool_name = getattr(action, 'tool', 'unknown')
                                    print(f"   Step {i+1}: Used tool '{tool_name}'")
                        
                        # ì¶œë ¥ ë¶„ì„
                        output = result.get("output", "")
                        
                        # ìµœì¢… ê²°ê³¼ íì— ì¶”ê°€
                        await callback_handler.stream_queue.put({
                            "type": "final_result",
                            "content": result["output"],
                            "intermediate_steps": result.get("intermediate_steps", []),
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        })
                        
                    except Exception as e:
                        print(f"âŒ Agent execution failed: {e}")
                        
                        # Rate limit ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬
                        error_message = str(e)
                        if "rate_limit_error" in error_message or "429" in error_message:
                            print("ğŸš« Rate limit exceeded - reducing token usage recommended")
                            error_message = "API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        
                        await callback_handler.stream_queue.put({
                            "type": "error",
                            "error": error_message,
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        })
                    finally:
                        # ì¢…ë£Œ ì‹ í˜¸
                        await callback_handler.stream_queue.put(None)
                
                # ì—ì´ì „íŠ¸ ì‹¤í–‰ íƒœìŠ¤í¬ ì‹œì‘
                agent_task = asyncio.create_task(run_agent())
                
                # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ yield
                first_chunk_time = None
                chunk_count = 0
                while True:
                    try:
                        # íƒ€ì„ì•„ì›ƒì„ ë‘ì–´ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
                        chunk = await asyncio.wait_for(
                            callback_handler.stream_queue.get(), 
                            timeout=60.0
                        )
                        
                        if chunk is None:  # ì¢…ë£Œ ì‹ í˜¸
                            break
                        
                        # ì²« ë²ˆì§¸ ì‘ë‹µ ì²­í¬ ì‹œê°„ ì¸¡ì •
                        if first_chunk_time is None and chunk.get("type") == "content":
                            first_chunk_time = time.time() - start_time
                            print(f"â±ï¸ First response chunk took: {first_chunk_time:.3f}s")
                        
                        chunk_count += 1
                        
                        yield chunk
                        
                    except asyncio.TimeoutError:
                        print("âš ï¸ Streaming timeout - sending error")
                        yield {
                            "type": "error",
                            "error": "Streaming timeout",
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        }
                        break
                    except Exception as e:
                        print(f"âŒ Error in streaming: {e}")
                        yield {
                            "type": "error",
                            "error": str(e),
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        }
                        break
                
                # ì—ì´ì „íŠ¸ íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
                try:
                    await agent_task
                except Exception as e:
                    print(f"âŒ Agent task error: {e}")
                
                total_time = time.time() - start_time
                print(f"â±ï¸ Total streaming function took: {total_time:.3f}s")
            
        except Exception as e:
            yield {
                "type": "error",
                "error": str(e),
                "message_id": message_id,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }
    
    def _prepare_messages(
        self,
        user_message: str,
        conversation_history: List[Dict[str, str]] = None
    ):
        """LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        messages = []
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
        messages.append(SystemMessage(content=get_en_system_prompt_with_tools()))
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
        if conversation_history:
            for msg in conversation_history:
                if msg.get("role") == "user":
                    messages.append(HumanMessage(content=msg["content"]))
                elif msg.get("role") == "assistant":
                    content = msg["content"]
                    
                    # tool ê²°ê³¼ê°€ í¬í•¨ëœ ê²½ìš° íŒŒì‹±
                    if "<!-- TOOL_RESULTS:" in content:
                        # ì‚¬ìš©ìì—ê²Œ ë³´ì´ëŠ” ë¶€ë¶„ê³¼ tool ì •ë³´ ë¶„ë¦¬
                        parts = content.split("<!-- TOOL_RESULTS:")
                        user_visible_content = parts[0].strip()
                        
                        if len(parts) > 1:
                            try:
                                tool_part = parts[1].split(" -->")[0].strip()
                                tool_results = json.loads(tool_part)
                                
                                # tool ê²°ê³¼ë¥¼ í¬í•¨í•œ í™•ì¥ëœ ë©”ì‹œì§€ ìƒì„±
                                enhanced_content = user_visible_content + "\n\n[Previous tool results:\n"
                                for tool_info in tool_results:
                                    enhanced_content += f"- {tool_info['tool']}: {tool_info['input']} â†’ {tool_info['result'][:200]}...\n"
                                enhanced_content += "]"
                                
                                messages.append(AIMessage(content=enhanced_content))
                            except:
                                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
                                messages.append(AIMessage(content=user_visible_content))
                        else:
                            messages.append(AIMessage(content=user_visible_content))
                    else:
                        messages.append(AIMessage(content=content))
        
        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages.append(HumanMessage(content=user_message))
        
        return messages
    
    def get_conversation_starter(self) -> str:
        """ëŒ€í™” ì‹œì‘ ë©”ì‹œì§€ ë°˜í™˜"""
        return get_conversation_starter()
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - MCP context managerê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬"""
        print("âœ… LLM service cleanup completed")


# ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ - ê°„ë‹¨í•œ ì‹±ê¸€í†¤
_langchain_service = None

async def get_langchain_service() -> LangChainLLMService:
    """ê¸€ë¡œë²Œ LangChain ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _langchain_service
    
    if _langchain_service is None:
        _langchain_service = LangChainLLMService()
        print("âœ… Single MCP server LangChain service created")
    
    return _langchain_service