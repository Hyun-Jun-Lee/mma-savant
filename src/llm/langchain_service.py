"""
LangChain ê¸°ë°˜ LLM ì„œë¹„ìŠ¤
ë‹¨ì¼ MCP ì„œë²„ìš© ìµœì í™”ëœ êµ¬í˜„
"""
import os
import asyncio
import uuid
import time
import json
from typing import List, Dict, Any, Optional, AsyncGenerator
from datetime import datetime
from traceback import format_exc
from contextlib import asynccontextmanager

from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage
from langchain_core.messages import HumanMessage
from langchain_mcp_adapters.tools import load_mcp_tools
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from config import Config
from llm.prompts.en_ver import get_en_system_prompt_with_tools
from llm.providers import get_anthropic_llm
from llm.callbacks import get_anthropic_callback_handler
from conversation.message_manager import ChatHistoryManager
from database.connection.postgres_conn import get_async_db_context
from common.logging_config import get_logger

LOGGER = get_logger(__name__)


def setup_langsmith_tracing():
    """LangSmith ì¶”ì  ìƒíƒœ í™•ì¸ ë° ë¡œê¹…"""
    if Config.LANGCHAIN_TRACING_V2:
        LOGGER.info(f"âœ… LangSmith tracing enabled for project: {Config.LANGCHAIN_PROJECT}")
        if not Config.LANGCHAIN_API_KEY:
            LOGGER.warning("âš ï¸ LANGCHAIN_API_KEY is not set - tracing may not work properly")
    else:
        LOGGER.info("âŒ LangSmith tracing disabled")




class LangChainLLMService:
    """LangChain ê¸°ë°˜ LLM ì„œë¹„ìŠ¤ - ë‹¨ì¼ MCP ì„œë²„ ìµœì í™”"""
    
    def __init__(self, max_cache_size: int = 100):
        # LangSmith ì¶”ì  ì„¤ì •
        setup_langsmith_tracing()
        
        self.history_manager = ChatHistoryManager(
            async_db_session_factory=get_async_db_context,
            max_cache_size=max_cache_size
        )
        
        # ë‹¨ì¼ MCP ì„œë²„ ì„¤ì •
        current_dir = os.path.dirname(os.path.abspath(__file__))
        src_dir = os.path.dirname(current_dir)
        mcp_server_path = os.path.join(src_dir, "tools", "mcp_server.py")
        
        self.server_params = StdioServerParameters(
            command="python",
            args=[mcp_server_path],
        )
        
        # MCP ë„êµ¬ ìºì‹±
        self._cached_tools = None
        self._tools_loading = False
    
    @asynccontextmanager
    async def _get_mcp_tools(self):
        """
        MCP ë„êµ¬ë“¤ì„ context managerë¡œ ì œê³µ (ì„¸ì…˜ ìœ ì§€)
        """
        print("ğŸ”„ Loading MCP tools...")
        async with stdio_client(self.server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # ì—°ê²° ì´ˆê¸°í™”
                await session.initialize()
                
                # ë„êµ¬ ë¡œë“œ
                tools = await load_mcp_tools(session)
                print(f"âœ… MCP Tools loaded: {len(tools)} tools")
                
                yield tools

    def _extract_safe_text_content(self, content) -> str:
        """
        LangChain Agent ì¶œë ¥ì—ì„œ ì•ˆì „í•˜ê²Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        Anthropic í† í° í˜•íƒœì™€ ê¸°íƒ€ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ í•„í„°ë§
        """
        if isinstance(content, str):
            return content
        
        elif isinstance(content, dict):
            # Anthropic í† í° í˜•íƒœ í•„í„°ë§
            if content.get('type') in ['tool_use', 'input_json_delta', 'tool_call', 'function_call']:
                LOGGER.debug(f"ğŸ”§ Filtered tool-related token: {content.get('type')}")
                return ""
            
            # íˆ´ ID í† í° í•„í„°ë§
            if 'id' in content and str(content.get('id', '')).startswith('toolu_'):
                LOGGER.debug(f"ğŸ”§ Filtered tool ID token: {content.get('id')}")
                return ""
            
            # í‘œì¤€ í…ìŠ¤íŠ¸ í•„ë“œë“¤ í™•ì¸ (ìš°ì„ ìˆœìœ„ ìˆœ)
            for text_field in ['content', 'text', 'message']:
                if text_field in content:
                    text_value = content[text_field]
                    if isinstance(text_value, str):
                        return text_value
                    else:
                        # ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
                        return self._extract_safe_text_content(text_value)
            
            # ì•Œë ¤ì§„ êµ¬ì¡°í™”ëœ í˜•íƒœë“¤ ë¡œê¹…
            if 'type' in content and 'index' in content:
                LOGGER.debug(f"ğŸ” Skipped structured token: {content}")
                return ""
            
            # ê¸°íƒ€ ë”•ì…”ë„ˆë¦¬ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
            LOGGER.debug(f"âš ï¸ Unknown dict structure converted to string: {content}")
            return str(content)
        
        elif isinstance(content, list):
            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê° í•­ëª©ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ì—¬ í•©ì¹¨
            texts = []
            for item in content:
                extracted = self._extract_safe_text_content(item)
                if extracted:
                    texts.append(extracted)
            return ''.join(texts)
        
        else:
            # ê¸°íƒ€ íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
            return str(content) if content is not None else ""

    async def generate_streaming_chat_response(
        self,
        user_message: str,
        conversation_history: List[Dict[str, str]] = None,
        session_id: Optional[str] = None,
        user_id: Optional[int] = None,
        ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‘ë‹µ ìƒì„±
        """
        if not session_id:
            LOGGER.error("Session ID is required for streaming chat response")
            raise ValueError("Session ID is required for streaming chat response")

        if not user_id:
            LOGGER.error("User ID is required for streaming chat response")
            raise ValueError("User ID is required for streaming chat response")

        message_id = str(uuid.uuid4())
        start_time = time.time()
        
        # LangSmith ë©”íƒ€ë°ì´í„° ì„¤ì •
        langsmith_metadata = {
            "user_id": user_id,
            "session_id": session_id,
            "message_id": message_id,
            "service": "mma-savant",
            "version": "1.0",
            "start_time": datetime.now().isoformat()
        }
        
        # LangSmith ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (ìë™ìœ¼ë¡œ ì¶”ì ë¨)
        if Config.LANGCHAIN_TRACING_V2:
            LOGGER.debug(f"LangSmith metadata prepared: {langsmith_metadata}")
        
        # Chat history ê°€ì ¸ì˜¤ê¸°
        try:
            history_start = time.time()
            history = await self.history_manager.get_session_history(session_id, user_id)
            history_time = time.time() - history_start
            LOGGER.info(f"â±ï¸ History loading: {history_time:.3f}s")
            LOGGER.info(f"ğŸ“š Loaded {len(history.messages)} messages from cache")
        except Exception as e:
            error_details = {
                "error_type": type(e).__name__,
                "error_message": str(e),
                "user_id": user_id,
                "session_id": session_id,
                "message_id": message_id,
                "timestamp": datetime.now().isoformat()
            }
            
            LOGGER.error(f"âŒ Error loading chat history: {e}", extra={"langsmith_metadata": langsmith_metadata})
            LOGGER.error(format_exc())
            
            # LangSmith ì—ëŸ¬ ì •ë³´ ë¡œê¹… (ìë™ìœ¼ë¡œ ì¶”ì ë¨)
            if Config.LANGCHAIN_TRACING_V2:
                LOGGER.error(f"LangSmith error details: {error_details}")
            
            yield {
                "type": "error",
                "error": f"Failed to load chat history: {str(e)}",
                "message_id": message_id,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }
            return

        # ë‹¨ì¼ MCP ì„œë²„ì—ì„œ ë„êµ¬ ë¡œë“œ
        try:
            mcp_start = time.time()
            async with self._get_mcp_tools() as tools:
                mcp_time = time.time() - mcp_start
                LOGGER.info(f"â±ï¸ MCP tools loading took: {mcp_time:.3f}s")
                LOGGER.info(f"ğŸ”§ Loaded {len(tools)} MCP tools")
                
                # ì—ì´ì „íŠ¸ ì„¤ì • ë° ìƒì„±
                try:
                    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± (chat_history í¬í•¨)
                    prompt = ChatPromptTemplate.from_messages([
                        ("system", get_en_system_prompt_with_tools()),
                        ("placeholder", "{chat_history}"),
                        ("human", "{input}"),
                        ("placeholder", "{agent_scratchpad}"),
                    ])
                    
                    
                    # ì½œë°± í•¸ë“¤ëŸ¬ ê°€ì ¸ì˜¤ê¸° (ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬ìš©)
                    callback_handler = get_anthropic_callback_handler(message_id, session_id)
                    
                    # LLM Providerë¥¼ í†µí•œ LLM ìƒì„±
                    streaming_llm = get_anthropic_llm(
                        callback_handler=callback_handler
                    )

                    # ì—ì´ì „íŠ¸ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° LLM ì‚¬ìš©)
                    agent = create_tool_calling_agent(streaming_llm, tools, prompt)

                    agent_executor = AgentExecutor(
                        agent=agent, 
                        tools=tools, 
                        verbose=True,
                        return_intermediate_steps=True,  
                        callbacks=[callback_handler]  
                    )

                    # íˆìŠ¤í† ë¦¬ ì§ì ‘ ê´€ë¦¬ - RunnableWithMessageHistory ì œê±°
                    # ìœ íš¨í•œ ë©”ì‹œì§€ë“¤ë§Œ í•„í„°ë§ (dict íƒ€ì… ì œì™¸)
                    valid_chat_history = []
                    for msg in history.messages:
                        if hasattr(msg, 'content') and hasattr(msg, 'type'):
                            valid_chat_history.append(msg)
                        else:
                            LOGGER.debug(f"Skipped invalid message: {type(msg)} - {msg}")
                    
                    LOGGER.info(f"ğŸ“š Using {len(valid_chat_history)} valid messages for context")
                    
                    LOGGER.info(f"ğŸ¤– Agent created with {len(tools)} tools")
                    
                except Exception as e:
                    LOGGER.error(f"âŒ Error creating agent: {e}")
                    LOGGER.error(format_exc())
                    yield {
                        "type": "error",
                        "error": f"Failed to create agent: {str(e)}",
                        "message_id": message_id,
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat()
                    }
                    return
                
                async def run_agent():
                    try:
                        LOGGER.info("ğŸš€ Starting agent execution...")
                        
                        agent_exec_start = time.time()
                        invoke_start = time.time()
                        
                        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                        user_msg = HumanMessage(content=user_message)
                        history.add_message(user_msg)
                        
                        response_content = ""
                        tool_results = []
                        
                        # ì§ì ‘ agent_executor í˜¸ì¶œ (íˆìŠ¤í† ë¦¬ í¬í•¨)
                        # AgentExecutorì— ì´ë¯¸ ì½œë°±ì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì¤‘ë³µ ì „ë‹¬ ë°©ì§€
                        async for chunk in agent_executor.astream(
                            {
                                "input": user_message,
                                "chat_history": valid_chat_history
                            }
                        ):
                            
                            if isinstance(chunk, dict):
                                if "output" in chunk:
                                    content = chunk["output"]
                                    extracted_text = self._extract_safe_text_content(content)
                                    if extracted_text:
                                        response_content += extracted_text
                                
                                if "intermediate_steps" in chunk:
                                    steps = chunk["intermediate_steps"]
                                    LOGGER.info(f"ğŸ”§ Found intermediate_steps: {len(steps)} steps")
                                    for step in steps:
                                        if len(step) >= 2:
                                            action, observation = step
                                            tool_result = {
                                                "tool": getattr(action, 'tool', 'unknown'),
                                                "input": str(action.tool_input),
                                                "result": str(observation)[:500]
                                            }
                                            tool_results.append(tool_result)
                        
                        invoke_time = time.time() - invoke_start
                        agent_exec_time = time.time() - agent_exec_start
                        
                        LOGGER.info("âœ… Agent execution completed")
                        LOGGER.info(f"â±ï¸ Chain execution took: {invoke_time:.3f}s")
                        LOGGER.info(f"â±ï¸ Total agent execution took: {agent_exec_time:.3f}s")
                        
                        # AI ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ë©”ëª¨ë¦¬ ì¦‰ì‹œ + DB ë°±ê·¸ë¼ìš´ë“œ)
                        if response_content:
                            # response_contentê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸ ë° ì •ë¦¬
                            if isinstance(response_content, str):
                                clean_content = response_content.strip()
                            else:
                                LOGGER.warning(f"âš ï¸ Non-string response_content: {type(response_content)} - {response_content}")
                                clean_content = self._extract_safe_text_content(response_content)
                            
                            if clean_content:
                                ai_message = AIMessage(
                                    content=clean_content,
                                    additional_kwargs={"tool_results": tool_results} if tool_results else {}
                                )
                                history.add_message(ai_message)
                                LOGGER.info(f"âœ… AI message added to history: {len(clean_content)} chars")
                            else:
                                LOGGER.warning("âš ï¸ Empty or invalid response content after cleaning - not adding to history")
                        
                        # ê²°ê³¼ ë¶„ì„ ë¡œê¹…
                        if tool_results:
                            LOGGER.info(f"ğŸ“Š Agent used {len(tool_results)} tools")
                            for i, tool_result in enumerate(tool_results):
                                LOGGER.info(f"   Step {i+1}: Used tool '{tool_result['tool']}'")
                        
                        # ìµœì¢… ê²°ê³¼ íì— ì¶”ê°€
                        final_result = {
                            "type": "final_result",
                            "content": response_content,
                            "tool_results": tool_results,
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat(),
                            "total_execution_time": agent_exec_time,
                            "langsmith_enabled": Config.LANGCHAIN_TRACING_V2
                        }
                        
                        # LangSmith ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ê°€
                        if Config.LANGCHAIN_TRACING_V2:
                            performance_metrics = {
                                "total_execution_time": agent_exec_time,
                                "chain_execution_time": invoke_time,
                                "tools_used_count": len(tool_results),
                                "response_length": len(response_content),
                                "user_id": user_id,
                                "session_id": session_id
                            }
                            LOGGER.info(f"LangSmith performance metrics: {performance_metrics}")
                            final_result["performance_metrics"] = performance_metrics
                        
                        await callback_handler.stream_queue.put(final_result)
                        
                    except Exception as e:
                        error_details = {
                            "error_type": type(e).__name__,
                            "error_message": str(e),
                            "user_id": user_id,
                            "session_id": session_id,
                            "message_id": message_id,
                            "timestamp": datetime.now().isoformat(),
                            "execution_phase": "agent_execution"
                        }
                        
                        LOGGER.error(f"âŒ Agent execution failed: {e}", extra={"langsmith_metadata": langsmith_metadata})
                        LOGGER.error(format_exc())
                        
                        # LangSmith ì—ëŸ¬ ì •ë³´ ë¡œê¹… (ìë™ìœ¼ë¡œ ì¶”ì ë¨)
                        if Config.LANGCHAIN_TRACING_V2:
                            LOGGER.error(f"LangSmith error details: {error_details}")
                        
                        # Rate limit ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬
                        error_message = str(e)
                        if "rate_limit_error" in error_message or "429" in error_message:
                            LOGGER.warning("ğŸš« Rate limit exceeded - reducing token usage recommended")
                            error_message = "API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        
                        await callback_handler.stream_queue.put({
                            "type": "error",
                            "error": error_message,
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat(),
                            "langsmith_enabled": Config.LANGCHAIN_TRACING_V2
                        })
                    finally:
                        # ì¢…ë£Œ ì‹ í˜¸
                        await callback_handler.stream_queue.put(None)
                
                # ì—ì´ì „íŠ¸ ì‹¤í–‰ íƒœìŠ¤í¬ ì‹œì‘
                agent_task = asyncio.create_task(run_agent())
                
                # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ yield
                first_chunk_time = None
                chunk_count = 0
                while True:
                    try:
                        # íƒ€ì„ì•„ì›ƒì„ ë‘ì–´ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
                        chunk = await asyncio.wait_for(
                            callback_handler.stream_queue.get(), 
                            timeout=60.0
                        )
                        
                        if chunk is None:  # ì¢…ë£Œ ì‹ í˜¸
                            break
                        
                        # ì²« ë²ˆì§¸ ì‘ë‹µ ì²­í¬ ì‹œê°„ ì¸¡ì •
                        if first_chunk_time is None and chunk.get("type") == "content":
                            first_chunk_time = time.time() - start_time
                            print(f"â±ï¸ First response chunk took: {first_chunk_time:.3f}s")
                        
                        chunk_count += 1
                        
                        yield chunk
                        
                    except asyncio.TimeoutError:
                        LOGGER.warning("âš ï¸ Streaming timeout - sending error")
                        yield {
                            "type": "error",
                            "error": "Streaming timeout",
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        }
                        break
                    except Exception as e:
                        LOGGER.error(f"âŒ Error in streaming: {e}")
                        LOGGER.error(format_exc())
                        yield {
                            "type": "error",
                            "error": str(e),
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        }
                        break
                
                try:
                    await agent_task
                except Exception as e:
                    LOGGER.error(f"âŒ Agent task error: {e}")
                    LOGGER.error(format_exc())
                
                total_time = time.time() - start_time
                LOGGER.info(f"â±ï¸ Total streaming function took: {total_time:.3f}s")
                
                # LangSmith ìµœì¢… ë©”íŠ¸ë¦­ ë¡œê¹… (ìë™ìœ¼ë¡œ ì¶”ì ë¨)
                if Config.LANGCHAIN_TRACING_V2:
                    final_metrics = {
                        "total_streaming_time": total_time,
                        "message_id": message_id,
                        "session_id": session_id,
                        "user_id": user_id,
                        "completion_status": "success"
                    }
                    LOGGER.info(f"LangSmith final metrics: {final_metrics}")
                
        except Exception as e:
            LOGGER.error(f"âŒ Error loading MCP tools: {e}")
            LOGGER.error(format_exc())
            yield {
                "type": "error",
                "error": f"Failed to load MCP tools: {str(e)}",
                "message_id": message_id,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }

    def get_conversation_starter(self) -> str:
        """ëŒ€í™” ì‹œì‘ ë©”ì‹œì§€ ë°˜í™˜"""
        return get_conversation_starter()
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - MCP context managerê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬"""
        LOGGER.info("âœ… LLM service cleanup completed")


# ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ - ê°„ë‹¨í•œ ì‹±ê¸€í†¤
_langchain_service = None

async def get_langchain_service() -> LangChainLLMService:
    """ê¸€ë¡œë²Œ LangChain ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _langchain_service
    
    if _langchain_service is None:
        _langchain_service = LangChainLLMService()
        LOGGER.info("âœ… Single MCP server LangChain service created")
    
    return _langchain_service