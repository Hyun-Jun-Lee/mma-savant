"""
LangChain ê¸°ë°˜ LLM ì„œë¹„ìŠ¤ V2 - ë¦¬íŒ©í† ë§ëœ ë²„ì „
ëª¨ë“ˆí™”ëœ ì•„í‚¤í…ì²˜ë¡œ ë‹¤ì–‘í•œ LLM í”„ë¡œë°”ì´ë” ì§€ì› ë° í–¥ìƒëœ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
"""
import os
import asyncio
import uuid
import time
from typing import List, Dict, Any, Optional, AsyncGenerator
from datetime import datetime
from traceback import format_exc

from langchain_core.messages import AIMessage, HumanMessage

from config import Config
from llm.model_factory import create_llm_with_callbacks, get_available_providers
from llm.agent_manager import AgentManager
from llm.stream_processor import (
    extract_safe_text_content, 
    clean_response_content,
    create_final_result,
    create_error_response,
    validate_streaming_chunk
)
from common.utils import remove_timestamps_from_tool_result
from llm.performance_monitor import setup_langsmith_tracing
from conversation.message_manager import ChatHistoryManager
from database.connection.postgres_conn import get_async_db_context
from common.logging_config import get_logger
from common.utils import kr_time_now

LOGGER = get_logger(__name__)


class LangChainLLMService:
    """
    LangChain LLM ì„œë¹„ìŠ¤ V2 - ëª¨ë“ˆí™”ëœ ì•„í‚¤í…ì²˜
    """
    
    def __init__(self, max_cache_size: int = 100, provider: Optional[str] = None):
        """
        ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        
        Args:
            max_cache_size: íˆìŠ¤í† ë¦¬ ìºì‹œ í¬ê¸°
            provider: ì‚¬ìš©í•  LLM í”„ë¡œë°”ì´ë” (Noneì´ë©´ Configì—ì„œ ê²°ì •)
        """
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° íŠ¸ë ˆì´ì‹± ì„¤ì •
        setup_langsmith_tracing()
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.history_manager = ChatHistoryManager(
            async_db_session_factory=get_async_db_context,
            max_cache_size=max_cache_size
        )
        
        self.agent_manager = AgentManager()
        
        # LLM í”„ë¡œë°”ì´ë” ì„¤ì •
        self.provider = provider or Config.LLM_PROVIDER
    
    async def generate_streaming_chat_response(
        self,
        user_message: str,
        session_id: Optional[str] = None,
        user_id: Optional[int] = None,
        provider_override: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‘ë‹µ ìƒì„±
        
        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            user_id: ì‚¬ìš©ì ID
            provider_override: í”„ë¡œë°”ì´ë” ì˜¤ë²„ë¼ì´ë“œ
        
        Yields:
            Dict[str, Any]: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬ë“¤
        """
        # í•„ìˆ˜ ë§¤ê°œë³€ìˆ˜ ê²€ì¦
        if not session_id:
            LOGGER.error("Session ID is required for streaming chat response")
            yield create_error_response(
                ValueError("Session ID is required"),
                "unknown",
                "unknown"
            )
            return

        if not user_id:
            LOGGER.error("User ID is required for streaming chat response")
            yield create_error_response(
                ValueError("User ID is required"),
                "unknown",
                session_id
            )
            return

        message_id = str(uuid.uuid4())
        start_time = time.time()
        
        # LangSmith ë©”íƒ€ë°ì´í„° ì„¤ì •
        langsmith_metadata = {
            "user_id": user_id,
            "session_id": session_id,
            "message_id": message_id,
            "service": "mma-savant",
            "version": "2.0",
            "start_time": kr_time_now().isoformat()
        }
        
        # LangSmith ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (ìë™ìœ¼ë¡œ ì¶”ì ë¨)
        if Config.LANGCHAIN_TRACING_V2:
            LOGGER.debug(f"LangSmith metadata prepared: {langsmith_metadata}")
        
        try:
            
            # 1. ì±„íŒ… íˆìŠ¤í† ë¦¬ ë¡œë“œ
            try:
                history_start = time.time()
                history = await self.history_manager.get_session_history(session_id, user_id)
                history_time = time.time() - history_start
                LOGGER.info(f"â±ï¸ History loading: {history_time:.3f}s")
                LOGGER.info(f"ğŸ“š Loaded {len(history.messages)} messages from cache")
                
            except Exception as e:
                LOGGER.error(f"âŒ Error loading chat history: {e}")
                LOGGER.error(format_exc())
                
                yield {
                    "type": "error",
                    "error": f"Failed to load chat history: {str(e)}",
                    "message_id": message_id,
                    "session_id": session_id,
                    "timestamp": kr_time_now().isoformat()
                }
                return
            
            # 2. MCP ë„êµ¬ ë¡œë“œ ë° LLM ì„¤ì •
            try:
                mcp_start = time.time()
                async with self.agent_manager.get_mcp_tools() as tools:
                    mcp_time = time.time() - mcp_start
                    LOGGER.info(f"â±ï¸ MCP tools loading took: {mcp_time:.3f}s")
                    LOGGER.info(f"ğŸ”§ Loaded {len(tools)} MCP tools")
                    
                    # ì—ì´ì „íŠ¸ ì„¤ì • ë° ìƒì„±
                    try:
                        # LLM ë° ì½œë°± ìƒì„±
                        selected_provider = provider_override or self.provider
                        llm, callback_handler = create_llm_with_callbacks(
                            message_id=message_id,
                            session_id=session_id,
                            provider=selected_provider
                        )
                        
                        LOGGER.info(f"ğŸ¤– Using provider: {selected_provider}")
                        
                        # íˆìŠ¤í† ë¦¬ ê²€ì¦ ë° ì—ì´ì „íŠ¸ ìƒì„±
                        valid_chat_history = self.agent_manager.validate_chat_history(history.messages)
                        agent = self.agent_manager.create_agent_with_tools(llm, tools, valid_chat_history)
                        agent_executor = self.agent_manager.create_agent_executor(
                            agent, tools, callback_handler
                        )
                        
                        LOGGER.info(f"ğŸ“š Using {len(valid_chat_history)} valid messages for context")
                        LOGGER.info(f"ğŸ¤– Agent created with {len(tools)} tools")
                        
                    except Exception as e:
                        LOGGER.error(f"âŒ Error creating agent: {e}")
                        LOGGER.error(format_exc())
                        yield {
                            "type": "error",
                            "error": f"Failed to create agent: {str(e)}",
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": kr_time_now().isoformat()
                        }
                        return
                
                    # 3. ì—ì´ì „íŠ¸ ì‹¤í–‰ ë° ìŠ¤íŠ¸ë¦¬ë°
                    async for chunk in self._execute_agent_with_streaming(
                        agent_executor=agent_executor,
                        user_message=user_message,
                        chat_history=valid_chat_history,
                        callback_handler=callback_handler,
                        history=history,
                        message_id=message_id,
                        session_id=session_id,
                        user_id=user_id
                    ):
                        # ì²­í¬ ìœ íš¨ì„± ê²€ì‚¬
                        if validate_streaming_chunk(chunk):
                            yield chunk
                        else:
                            LOGGER.warning(f"âš ï¸ Invalid streaming chunk filtered: {chunk}")
            
            except Exception as e:
                LOGGER.error(f"âŒ Error loading MCP tools: {e}")
                LOGGER.error(format_exc())
                yield {
                    "type": "error",
                    "error": f"Failed to load MCP tools: {str(e)}",
                    "message_id": message_id,
                    "session_id": session_id,
                    "timestamp": kr_time_now().isoformat()
                }
        
        except Exception as e:
            LOGGER.error(f"âŒ Main execution error: {e}")
            LOGGER.error(format_exc())
            yield {
                "type": "error", 
                "error": str(e),
                "message_id": message_id,
                "session_id": session_id,
                "timestamp": kr_time_now().isoformat()
            }
        
        finally:
            # ì´ ì‹¤í–‰ ì‹œê°„ ë¡œê¹…
            total_time = time.time() - start_time
            LOGGER.info(f"â±ï¸ Total streaming function took: {total_time:.3f}s")
            
            # LangSmith ìµœì¢… ë©”íŠ¸ë¦­ ë¡œê¹… (ìë™ìœ¼ë¡œ ì¶”ì ë¨)
            if Config.LANGCHAIN_TRACING_V2:
                final_metrics = {
                    "total_streaming_time": total_time,
                    "message_id": message_id,
                    "session_id": session_id,
                    "user_id": user_id,
                    "completion_status": "success"
                }
                LOGGER.info(f"LangSmith final metrics: {final_metrics}")
    
    async def _execute_agent_with_streaming(
        self,
        agent_executor,
        user_message: str,
        chat_history: List,
        callback_handler,
        history,
        message_id: str,
        session_id: str,
        user_id: Optional[int] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ì—ì´ì „íŠ¸ ì‹¤í–‰ ë° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
        """
        response_content = ""
        tool_results = []
        
        async def run_agent():
            nonlocal response_content, tool_results
            
            try:
                agent_exec_start = time.time()
                LOGGER.info("ğŸš€ Starting agent execution...")
                
                # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                user_msg = HumanMessage(content=user_message)
                history.add_message(user_msg)
                
                # ì—ì´ì „íŠ¸ ì‹¤í–‰ ì„¤ì •
                execution_config = self.agent_manager.create_execution_config(
                    user_message=user_message,
                    chat_history=chat_history
                )
                
                # ì—ì´ì „íŠ¸ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ (v1ê³¼ ë™ì¼í•œ ë°©ì‹)
                async for chunk in agent_executor.astream(execution_config):
                    if isinstance(chunk, dict):
                        if "output" in chunk:
                            content = chunk["output"]
                            extracted_text = extract_safe_text_content(content)
                            if extracted_text:
                                response_content += extracted_text
                        
                        if "intermediate_steps" in chunk:
                            steps = chunk["intermediate_steps"]
                            LOGGER.info(f"ğŸ”§ Found intermediate_steps: {len(steps)} steps")
                            for step in steps:
                                if len(step) >= 2:
                                    action, observation = step
                                    tool_result = {
                                        "tool": getattr(action, 'tool', 'unknown'),
                                        "input": str(action.tool_input),
                                        "result": str(remove_timestamps_from_tool_result(observation))
                                    }
                                    tool_results.append(tool_result)
                
                agent_exec_time = time.time() - agent_exec_start
                LOGGER.info("âœ… Agent execution completed")
                LOGGER.info(f"â±ï¸ Total agent execution took: {agent_exec_time:.3f}s")
                
                # AI ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ë©”ëª¨ë¦¬ ì¦‰ì‹œ + DB ë°±ê·¸ë¼ìš´ë“œ)
                if response_content:
                    # response_contentê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸ ë° ì •ë¦¬
                    if isinstance(response_content, str):
                        clean_content = response_content.strip()
                    else:
                        LOGGER.warning(f"âš ï¸ Non-string response_content: {type(response_content)} - {response_content}")
                        clean_content = extract_safe_text_content(response_content)
                    
                    if clean_content:
                        ai_message = AIMessage(
                            content=clean_content,
                            additional_kwargs={"tool_results": tool_results if tool_results else []}
                        )
                        history.add_message(ai_message)
                        LOGGER.info(f"âœ… AI message added to history: {len(clean_content)} chars")
                    else:
                        LOGGER.warning("âš ï¸ Empty or invalid response content after cleaning - not adding to history")
                
                # ê²°ê³¼ ë¶„ì„ ë¡œê¹…
                if tool_results:
                    LOGGER.info(f"ğŸ“Š Agent used {len(tool_results)} tools")
                    for i, tool_result in enumerate(tool_results):
                        LOGGER.info(f"   Step {i+1}: Used tool '{tool_result['tool']}'")
                
                # ìµœì¢… ê²°ê³¼ íì— ì¶”ê°€
                final_result = {
                    "type": "final_result",
                    "content": response_content,
                    "tool_results": tool_results,
                    "message_id": message_id,
                    "session_id": session_id,
                    "timestamp": kr_time_now().isoformat(),
                    "total_execution_time": agent_exec_time,
                    "langsmith_enabled": Config.LANGCHAIN_TRACING_V2
                }
                
                # LangSmith ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ê°€
                if Config.LANGCHAIN_TRACING_V2:
                    performance_metrics = {
                        "total_execution_time": agent_exec_time,
                        "tools_used_count": len(tool_results),
                        "response_length": len(response_content),
                        "user_id": user_id,
                        "session_id": session_id
                    }
                    LOGGER.info(f"LangSmith performance metrics: {performance_metrics}")
                    final_result["performance_metrics"] = performance_metrics
                
                await callback_handler.stream_queue.put(final_result)
                    
            except Exception as e:
                LOGGER.error(f"âŒ Agent execution failed: {e}")
                LOGGER.error(format_exc())
                
                # Rate limit ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬
                error_message = str(e)
                if "rate_limit_error" in error_message or "429" in error_message:
                    LOGGER.warning("ğŸš« Rate limit exceeded - reducing token usage recommended")
                    error_message = "API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                
                await callback_handler.stream_queue.put({
                    "type": "error",
                    "error": error_message,
                    "message_id": message_id,
                    "session_id": session_id,
                    "timestamp": kr_time_now().isoformat(),
                    "langsmith_enabled": Config.LANGCHAIN_TRACING_V2
                })
            
            finally:
                # ì¢…ë£Œ ì‹ í˜¸
                await callback_handler.stream_queue.put(None)
        
        # ì—ì´ì „íŠ¸ ì‹¤í–‰ íƒœìŠ¤í¬ ì‹œì‘
        agent_task = asyncio.create_task(run_agent())
        
        # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ yield
        first_chunk_time = None
        chunk_count = 0
        
        while True:
            try:
                # íƒ€ì„ì•„ì›ƒì„ ë‘ì–´ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
                chunk = await asyncio.wait_for(
                    callback_handler.stream_queue.get(), 
                    timeout=60.0
                )
                
                if chunk is None:  # ì¢…ë£Œ ì‹ í˜¸
                    break
                
                # ì²« ë²ˆì§¸ ì‘ë‹µ ì²­í¬ ì‹œê°„ ì¸¡ì •
                if first_chunk_time is None and chunk.get("type") == "content":
                    first_chunk_time = time.time()
                    LOGGER.debug("âš¡ First response chunk received")
                
                chunk_count += 1
                yield chunk
                
            except asyncio.TimeoutError:
                LOGGER.warning("âš ï¸ Streaming timeout - sending error")
                yield {
                    "type": "error",
                    "error": "Streaming timeout",
                    "message_id": message_id,
                    "session_id": session_id,
                    "timestamp": kr_time_now().isoformat()
                }
                break
            
            except Exception as e:
                LOGGER.error(f"âŒ Error in streaming: {e}")
                LOGGER.error(format_exc())
                yield {
                    "type": "error",
                    "error": str(e),
                    "message_id": message_id,
                    "session_id": session_id,
                    "timestamp": kr_time_now().isoformat()
                }
                break
        
        try:
            await agent_task
        except Exception as e:
            LOGGER.error(f"âŒ Agent task error: {e}")
    
    def get_conversation_starter(self) -> str:
        """ëŒ€í™” ì‹œì‘ ë©”ì‹œì§€ ë°˜í™˜"""
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë”ì— ë”°ë¥¸ ë§ì¶¤ ë©”ì‹œì§€
            available_providers = get_available_providers()
            provider_info = f" (Using {self.provider})" if len(available_providers) > 1 else ""
            
            return f"ì•ˆë…•í•˜ì„¸ìš”! MMA Savantì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤{provider_info}. MMAì— ê´€í•œ ëª¨ë“  ê²ƒì„ ë¬¼ì–´ë³´ì„¸ìš”!"
            
        except Exception as e:
            LOGGER.error(f"âŒ Error getting conversation starter: {e}")
            return "ì•ˆë…•í•˜ì„¸ìš”! MMAì— ê´€í•œ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."
    
    async def health_check(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            # ê¸°ë³¸ ìƒíƒœ
            health_status = {
                "service": "LangChainLLMServiceV2",
                "status": "healthy",
                "timestamp": kr_time_now().isoformat(),
                "version": "2.0"
            }
            
            # í”„ë¡œë°”ì´ë” ìƒíƒœ
            available_providers = get_available_providers()
            health_status.update({
                "llm_provider": self.provider,
                "available_providers": available_providers,
                "providers_count": len(available_providers)
            })
            
            # ì—ì´ì „íŠ¸ ë§¤ë‹ˆì € ìƒíƒœ
            agent_health = await self.agent_manager.health_check()
            health_status["agent_manager"] = agent_health
            
            # LangSmith ìƒíƒœ
            health_status["langsmith_enabled"] = Config.LANGCHAIN_TRACING_V2
            
            return health_status
            
        except Exception as e:
            LOGGER.error(f"âŒ Health check error: {e}")
            return {
                "service": "LangChainLLMServiceV2",
                "status": "error",
                "error": str(e),
                "timestamp": kr_time_now().isoformat()
            }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ì—ì´ì „íŠ¸ ë§¤ë‹ˆì € ì •ë¦¬
            if hasattr(self, 'agent_manager'):
                self.agent_manager.clear_tools_cache()
            
            LOGGER.info("âœ… LLM service V2 cleanup completed")
            
        except Exception as e:
            LOGGER.error(f"âŒ Cleanup error: {e}")


# ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
_langchain_service_v2 = None


async def get_langchain_service(
    provider: Optional[str] = None,
    max_cache_size: int = 100
) -> LangChainLLMService:
    """
    ê¸€ë¡œë²Œ LangChain ì„œë¹„ìŠ¤ V2 ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    Args:
        provider: LLM í”„ë¡œë°”ì´ë” (ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œì—ë§Œ ì ìš©)
        max_cache_size: ìºì‹œ í¬ê¸° (ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œì—ë§Œ ì ìš©)
    
    Returns:
        LangChainLLMService: ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
    """
    global _langchain_service_v2
    
    if _langchain_service_v2 is None:
        _langchain_service_v2 = LangChainLLMService(
            max_cache_size=max_cache_size,
            provider=provider
        )
        LOGGER.info("âœ… LangChain service V2 created")
    
    return _langchain_service_v2


# í¸ì˜ í•¨ìˆ˜ë“¤
async def create_streaming_response(
    user_message: str,
    session_id: str,
    user_id: int,
    provider: Optional[str] = None
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± í¸ì˜ í•¨ìˆ˜
    
    Args:
        user_message: ì‚¬ìš©ì ë©”ì‹œì§€
        session_id: ì„¸ì…˜ ID
        user_id: ì‚¬ìš©ì ID
        provider: í”„ë¡œë°”ì´ë” ì˜¤ë²„ë¼ì´ë“œ
    
    Yields:
        Dict[str, Any]: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²­í¬ë“¤
    """
    service = await get_langchain_service(provider=provider)
    
    async for chunk in service.generate_streaming_chat_response(
        user_message=user_message,
        session_id=session_id,
        user_id=user_id,
        provider_override=provider
    ):
        yield chunk


if __name__ == "__main__":
    """í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹…ìš©"""
    import asyncio
    
    async def test_service_v2():
        print("ğŸš€ LangChain Service V2 Test")
        print("=" * 50)
        
        try:
            # ì„œë¹„ìŠ¤ ìƒì„±
            service = await get_langchain_service()
            
            # ìƒíƒœ í™•ì¸
            health = await service.health_check()
            print(f"\nğŸ¥ Health Check:")
            print(f"  Status: {health['status']}")
            print(f"  Provider: {health['llm_provider']}")
            print(f"  Available Providers: {health['available_providers']}")
            
            # ëŒ€í™” ì‹œì‘ ë©”ì‹œì§€
            starter = service.get_conversation_starter()
            print(f"\nğŸ’¬ Conversation Starter:")
            print(f"  {starter}")
            
            print(f"\nâœ… Service V2 test completed successfully")
            
        except Exception as e:
            print(f"\nâŒ Service V2 test failed: {e}")
            import traceback
            traceback.print_exc()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    if asyncio.get_event_loop().is_running():
        print("Running in existing event loop - skipping test")
    else:
        asyncio.run(test_service_v2())