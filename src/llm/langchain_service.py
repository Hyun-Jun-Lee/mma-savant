"""
LangChain ê¸°ë°˜ LLM ì„œë¹„ìŠ¤
ë‹¨ì¼ MCP ì„œë²„ìš© ìµœì í™”ëœ êµ¬í˜„
"""
import os
import asyncio
import uuid
import time
import json
from typing import List, Dict, Any, Optional, AsyncGenerator
from datetime import datetime
from traceback import format_exc
from contextlib import asynccontextmanager

from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import AIMessage
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_mcp_adapters.tools import load_mcp_tools
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

from llm.prompts.en_ver import get_en_system_prompt_with_tools
from llm.providers import get_anthropic_llm
from llm.callbacks import get_anthropic_callback_handler
from conversation.message_manager import ChatHistoryManager
from database.connection.postgres_conn import get_async_db_context
from common.logging_config import get_logger

LOGGER = get_logger(__name__)




class LangChainLLMService:
    """LangChain ê¸°ë°˜ LLM ì„œë¹„ìŠ¤ - ë‹¨ì¼ MCP ì„œë²„ ìµœì í™”"""
    
    def __init__(self, max_cache_size: int = 100):
        self.history_manager = ChatHistoryManager(
            async_db_session_factory=get_async_db_context,
            max_cache_size=max_cache_size
        )
        
        # ë‹¨ì¼ MCP ì„œë²„ ì„¤ì •
        current_dir = os.path.dirname(os.path.abspath(__file__))
        src_dir = os.path.dirname(current_dir)
        mcp_server_path = os.path.join(src_dir, "tools", "mcp_server.py")
        
        self.server_params = StdioServerParameters(
            command="python",
            args=[mcp_server_path],
        )
        
        # MCP ë„êµ¬ ìºì‹±
        self._cached_tools = None
        self._tools_loading = False
    
    @asynccontextmanager
    async def _get_mcp_tools(self):
        """
        MCP ë„êµ¬ë“¤ì„ context managerë¡œ ì œê³µ (ì„¸ì…˜ ìœ ì§€)
        """
        print("ğŸ”„ Loading MCP tools...")
        async with stdio_client(self.server_params) as (read, write):
            async with ClientSession(read, write) as session:
                # ì—°ê²° ì´ˆê¸°í™”
                await session.initialize()
                
                # ë„êµ¬ ë¡œë“œ
                tools = await load_mcp_tools(session)
                print(f"âœ… MCP Tools loaded: {len(tools)} tools")
                
                yield tools

    async def generate_streaming_chat_response(
        self,
        user_message: str,
        conversation_history: List[Dict[str, str]] = None,
        session_id: Optional[str] = None,
        user_id: Optional[int] = None,
        ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… ì‘ë‹µ ìƒì„±
        """
        if not session_id:
            LOGGER.error("Session ID is required for streaming chat response")
            raise ValueError("Session ID is required for streaming chat response")

        if not user_id:
            LOGGER.error("User ID is required for streaming chat response")
            raise ValueError("User ID is required for streaming chat response")

        message_id = str(uuid.uuid4())
        start_time = time.time()
        
        # Chat history ê°€ì ¸ì˜¤ê¸°
        try:
            history_start = time.time()
            history = await self.history_manager.get_session_history(session_id, user_id)
            history_time = time.time() - history_start
            LOGGER.info(f"â±ï¸ History loading: {history_time:.3f}s")
            LOGGER.info(f"ğŸ“š Loaded {len(history.messages)} messages from cache")
        except Exception as e:
            LOGGER.error(f"âŒ Error loading chat history: {e}")
            LOGGER.error(format_exc())
            yield {
                "type": "error",
                "error": f"Failed to load chat history: {str(e)}",
                "message_id": message_id,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }
            return

        # ë‹¨ì¼ MCP ì„œë²„ì—ì„œ ë„êµ¬ ë¡œë“œ
        try:
            mcp_start = time.time()
            async with self._get_mcp_tools() as tools:
                mcp_time = time.time() - mcp_start
                LOGGER.info(f"â±ï¸ MCP tools loading took: {mcp_time:.3f}s")
                LOGGER.info(f"ğŸ”§ Loaded {len(tools)} MCP tools")
                
                # ì—ì´ì „íŠ¸ ì„¤ì • ë° ìƒì„±
                try:
                    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
                    prompt = ChatPromptTemplate.from_messages([
                        ("system", get_en_system_prompt_with_tools()),
                        ("human", "{input}"),
                        ("placeholder", "{agent_scratchpad}"),
                    ])
                    
                    
                    # ì½œë°± í•¸ë“¤ëŸ¬ ê°€ì ¸ì˜¤ê¸° (ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ ì²˜ë¦¬ìš©)
                    callback_handler = get_anthropic_callback_handler(message_id, session_id)
                    
                    # LLM Providerë¥¼ í†µí•œ LLM ìƒì„±
                    streaming_llm = get_anthropic_llm(
                        callback_handler=callback_handler
                    )

                    # ì—ì´ì „íŠ¸ ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° LLM ì‚¬ìš©)
                    agent = create_tool_calling_agent(streaming_llm, tools, prompt)

                    agent_executor = AgentExecutor(
                        agent=agent, 
                        tools=tools, 
                        verbose=True,
                        return_intermediate_steps=True,  
                        callbacks=[callback_handler]  
                    )

                    # íˆìŠ¤í† ë¦¬ì™€ í•¨ê»˜ ì‹¤í–‰í•˜ëŠ” ì²´ì¸ ìƒì„±
                    def get_session_history_sync(sid: str):
                        LOGGER.debug(f"get_session_history_sync called with sid: {sid}")
                        return history  # ì´ë¯¸ ë¡œë“œëœ íˆìŠ¤í† ë¦¬ ë°˜í™˜
                    
                    chain_with_history = RunnableWithMessageHistory(
                        agent_executor,
                        get_session_history_sync,
                        input_messages_key="input",
                        history_messages_key="chat_history"
                    )
                    
                    LOGGER.info(f"ğŸ¤– Agent created with {len(tools)} tools")
                    
                except Exception as e:
                    LOGGER.error(f"âŒ Error creating agent: {e}")
                    LOGGER.error(format_exc())
                    yield {
                        "type": "error",
                        "error": f"Failed to create agent: {str(e)}",
                        "message_id": message_id,
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat()
                    }
                    return
                
                async def run_agent():
                    try:
                        LOGGER.info("ğŸš€ Starting agent execution...")
                        
                        agent_exec_start = time.time()
                        invoke_start = time.time()
                        
                        # RunnableWithMessageHistoryë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰
                        config = {"configurable": {"session_id": session_id}}
                        response_content = ""
                        tool_results = []
                        
                        # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê²°ê³¼ ì²˜ë¦¬
                        async for chunk in chain_with_history.astream(
                            {"input": user_message},
                            config=config
                        ):
                            if isinstance(chunk, dict):
                                if "output" in chunk:
                                    content = chunk["output"]
                                    # íƒ€ì… ì•ˆì „ì„± í™•ë³´
                                    if isinstance(content, str):
                                        response_content += content
                                    elif isinstance(content, list):
                                        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê° í•­ëª© ì²˜ë¦¬
                                        for item in content:
                                            if isinstance(item, dict) and 'text' in item:
                                                response_content += item['text']
                                            else:
                                                response_content += str(item)
                                    elif isinstance(content, dict):
                                        # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° text í•„ë“œ ì¶”ì¶œ
                                        if 'text' in content:
                                            response_content += content['text']
                                        else:
                                            response_content += str(content)
                                    else:
                                        # ê¸°íƒ€ íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
                                        response_content += str(content)
                                
                                if "intermediate_steps" in chunk:
                                    steps = chunk["intermediate_steps"]
                                    LOGGER.info(f"ğŸ”§ Found intermediate_steps: {len(steps)} steps")
                                    for step in steps:
                                        if len(step) >= 2:
                                            action, observation = step
                                            tool_result = {
                                                "tool": getattr(action, 'tool', 'unknown'),
                                                "input": str(action.tool_input),
                                                "result": str(observation)[:500]
                                            }
                                            tool_results.append(tool_result)
                                            LOGGER.debug(f"Added tool result: {tool_result['tool']}")
                        
                        invoke_time = time.time() - invoke_start
                        agent_exec_time = time.time() - agent_exec_start
                        
                        LOGGER.info("âœ… Agent execution completed")
                        LOGGER.info(f"â±ï¸ Chain execution took: {invoke_time:.3f}s")
                        LOGGER.info(f"â±ï¸ Total agent execution took: {agent_exec_time:.3f}s")
                        
                        # AI ì‘ë‹µì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ë©”ëª¨ë¦¬ ì¦‰ì‹œ + DB ë°±ê·¸ë¼ìš´ë“œ)
                        if response_content:
                            
                            ai_message = AIMessage(
                                content=response_content,
                                additional_kwargs={"tool_results": tool_results} if tool_results else {}
                            )
                            history.add_message(ai_message)
                        
                        # ê²°ê³¼ ë¶„ì„ ë¡œê¹…
                        if tool_results:
                            LOGGER.info(f"ğŸ“Š Agent used {len(tool_results)} tools")
                            for i, tool_result in enumerate(tool_results):
                                LOGGER.info(f"   Step {i+1}: Used tool '{tool_result['tool']}'")
                        
                        # ìµœì¢… ê²°ê³¼ íì— ì¶”ê°€
                        await callback_handler.stream_queue.put({
                            "type": "final_result",
                            "content": response_content,
                            "tool_results": tool_results,
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        })
                        
                    except Exception as e:
                        LOGGER.error(f"âŒ Agent execution failed: {e}")
                        LOGGER.error(format_exc())
                        
                        # Rate limit ì—ëŸ¬ íŠ¹ë³„ ì²˜ë¦¬
                        error_message = str(e)
                        if "rate_limit_error" in error_message or "429" in error_message:
                            LOGGER.warning("ğŸš« Rate limit exceeded - reducing token usage recommended")
                            error_message = "API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                        
                        await callback_handler.stream_queue.put({
                            "type": "error",
                            "error": error_message,
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        })
                    finally:
                        # ì¢…ë£Œ ì‹ í˜¸
                        await callback_handler.stream_queue.put(None)
                
                # ì—ì´ì „íŠ¸ ì‹¤í–‰ íƒœìŠ¤í¬ ì‹œì‘
                agent_task = asyncio.create_task(run_agent())
                
                # ìŠ¤íŠ¸ë¦¬ë° ì´ë²¤íŠ¸ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ yield
                first_chunk_time = None
                chunk_count = 0
                while True:
                    try:
                        # íƒ€ì„ì•„ì›ƒì„ ë‘ì–´ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
                        chunk = await asyncio.wait_for(
                            callback_handler.stream_queue.get(), 
                            timeout=60.0
                        )
                        
                        if chunk is None:  # ì¢…ë£Œ ì‹ í˜¸
                            break
                        
                        # ì²« ë²ˆì§¸ ì‘ë‹µ ì²­í¬ ì‹œê°„ ì¸¡ì •
                        if first_chunk_time is None and chunk.get("type") == "content":
                            first_chunk_time = time.time() - start_time
                            print(f"â±ï¸ First response chunk took: {first_chunk_time:.3f}s")
                        
                        chunk_count += 1
                        
                        yield chunk
                        
                    except asyncio.TimeoutError:
                        LOGGER.warning("âš ï¸ Streaming timeout - sending error")
                        yield {
                            "type": "error",
                            "error": "Streaming timeout",
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        }
                        break
                    except Exception as e:
                        LOGGER.error(f"âŒ Error in streaming: {e}")
                        LOGGER.error(format_exc())
                        yield {
                            "type": "error",
                            "error": str(e),
                            "message_id": message_id,
                            "session_id": session_id,
                            "timestamp": datetime.now().isoformat()
                        }
                        break
                
                try:
                    await agent_task
                except Exception as e:
                    LOGGER.error(f"âŒ Agent task error: {e}")
                    LOGGER.error(format_exc())
                
                total_time = time.time() - start_time
                LOGGER.info(f"â±ï¸ Total streaming function took: {total_time:.3f}s")
                
        except Exception as e:
            LOGGER.error(f"âŒ Error loading MCP tools: {e}")
            LOGGER.error(format_exc())
            yield {
                "type": "error",
                "error": f"Failed to load MCP tools: {str(e)}",
                "message_id": message_id,
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }

    def get_conversation_starter(self) -> str:
        """ëŒ€í™” ì‹œì‘ ë©”ì‹œì§€ ë°˜í™˜"""
        return get_conversation_starter()
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - MCP context managerê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬"""
        LOGGER.info("âœ… LLM service cleanup completed")


# ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ - ê°„ë‹¨í•œ ì‹±ê¸€í†¤
_langchain_service = None

async def get_langchain_service() -> LangChainLLMService:
    """ê¸€ë¡œë²Œ LangChain ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _langchain_service
    
    if _langchain_service is None:
        _langchain_service = LangChainLLMService()
        LOGGER.info("âœ… Single MCP server LangChain service created")
    
    return _langchain_service