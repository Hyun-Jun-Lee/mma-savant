"""
LLM API í´ë¼ì´ì–¸íŠ¸
Claude/OpenAI API í†µí•© ì²˜ë¦¬
"""
import os
import json
import asyncio
from typing import AsyncGenerator, Dict, List, Any, Optional
from datetime import datetime

import httpx
from anthropic import AsyncAnthropic

from config import Config


class LLMConfig:
    """LLM ì„¤ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.anthropic_api_key = Config.ANTHROPIC_API_KEY
        
        # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
        self.default_model = Config.ANTHROPIC_MODEL_NAME
        self.max_tokens = 4000
        self.temperature = 0.7
        
        # ì¬ì‹œë„ ì„¤ì •
        self.max_retries = 3
        self.retry_delay = 1.0
        
        # ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •
        self.stream_chunk_size = 50  # ë¬¸ì ë‹¨ìœ„ë¡œ ì²­í¬ í¬ê¸°


class LLMClient:
    """LLM API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, config: Optional[LLMConfig] = None):
        self.config = config or LLMConfig()
        
        # Claude í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if self.config.anthropic_api_key:
            self.anthropic = AsyncAnthropic(api_key=self.config.anthropic_api_key)
        else:
            print("âŒ Claude API key not configured")
            self.anthropic = None
            
        # ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.usage_stats = {
            "total_requests": 0,
            "total_tokens": 0,
            "total_cost": 0.0
        }
    
    async def generate_response(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        tools: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„± (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
        """
        if not self.anthropic:
            raise ValueError("Claude API key not configured")
        
        model = model or self.config.default_model
        max_tokens = max_tokens or self.config.max_tokens
        temperature = temperature or self.config.temperature
        
        try:
            # Claude API í˜¸ì¶œ
            response = await self.anthropic.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt,
                messages=messages,
                tools=tools if tools else []
            )
            
            # ì‚¬ìš©ëŸ‰ ì¶”ì 
            self.usage_stats["total_requests"] += 1
            if hasattr(response, 'usage'):
                self.usage_stats["total_tokens"] += response.usage.input_tokens + response.usage.output_tokens
            
            return {
                "content": response.content[0].text if response.content else "",
                "model": model,
                "usage": {
                    "input_tokens": response.usage.input_tokens if hasattr(response, 'usage') else 0,
                    "output_tokens": response.usage.output_tokens if hasattr(response, 'usage') else 0,
                },
                "tool_calls": self._extract_tool_calls(response),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            raise LLMError(f"Failed to generate response: {str(e)}")
    
    async def generate_streaming_response(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        model: Optional[str] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        tools: Optional[List[Dict]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        """
        if not self.anthropic:
            raise ValueError("Claude API key not configured")
        
        model = model or self.config.default_model
        max_tokens = max_tokens or self.config.max_tokens
        temperature = temperature or self.config.temperature
        
        try:
            print(f"ğŸ”„ Calling Claude API: model={model}, messages={len(messages)}, tools={len(tools) if tools else 0}")
            
            # Claude ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
            async with self.anthropic.messages.stream(
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt,
                messages=messages,
                tools=tools if tools else []
            ) as stream:
                
                async for chunk in stream:
                    # ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©
                    if chunk.type == "content_block_delta":
                        if chunk.delta.type == "text_delta":
                            yield {
                                "type": "content",
                                "content": chunk.delta.text,
                                "model": model,
                                "timestamp": datetime.now().isoformat()
                            }
                    
                    # ë©”ì‹œì§€ ì‹œì‘ ì‹ í˜¸
                    elif chunk.type == "message_start":
                        print(f"ğŸš€ Claude response started")
                        yield {
                            "type": "start",
                            "model": model,
                            "timestamp": datetime.now().isoformat()
                        }
                    
                    # ë©”ì‹œì§€ ì™„ë£Œ ì‹ í˜¸
                    elif chunk.type == "message_stop":
                        print(f"âœ… Claude response completed")
                        # ì‚¬ìš©ëŸ‰ ì¶”ì 
                        self.usage_stats["total_requests"] += 1
                        yield {
                            "type": "end",
                            "model": model,
                            "timestamp": datetime.now().isoformat()
                        }
                        
        except Exception as e:
            print(f"âŒ Claude API error: {e}")
            yield {
                "type": "error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    async def execute_with_retry(
        self,
        func,
        *args,
        max_retries: Optional[int] = None,
        **kwargs
    ):
        """
        ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ í•¨ìˆ˜ ì‹¤í–‰
        """
        max_retries = max_retries or self.config.max_retries
        last_error = None
        
        for attempt in range(max_retries + 1):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                last_error = e
                if attempt < max_retries:
                    await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                    continue
                break
        
        raise LLMError(f"Failed after {max_retries + 1} attempts: {str(last_error)}")
    
    def _extract_tool_calls(self, response) -> List[Dict]:
        """
        ì‘ë‹µì—ì„œ tool call ì¶”ì¶œ
        """
        tool_calls = []
        
        if hasattr(response, 'content'):
            for content_block in response.content:
                if hasattr(content_block, 'type') and content_block.type == "tool_use":
                    tool_calls.append({
                        "id": content_block.id,
                        "name": content_block.name,
                        "input": content_block.input
                    })
        
        return tool_calls
    
    def get_usage_stats(self) -> Dict[str, Any]:
        """
        ì‚¬ìš©ëŸ‰ í†µê³„ ë°˜í™˜
        """
        return self.usage_stats.copy()
    
    def reset_usage_stats(self):
        """
        ì‚¬ìš©ëŸ‰ í†µê³„ ë¦¬ì…‹
        """
        self.usage_stats = {
            "total_requests": 0,
            "total_tokens": 0,
            "total_cost": 0.0
        }


class LLMError(Exception):
    """LLM ê´€ë ¨ ì—ëŸ¬"""
    pass


# ê¸€ë¡œë²Œ í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
_llm_client = None

def get_llm_client() -> LLMClient:
    """
    ê¸€ë¡œë²Œ LLM í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    """
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client