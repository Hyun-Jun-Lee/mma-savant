import asyncio
import time
from typing import Dict, Any, List
from datetime import datetime

from langchain.callbacks.base import AsyncCallbackHandler
from langchain.schema import LLMResult

from common.utils import kr_time_now


class HuggingFaceCallbackHandler(AsyncCallbackHandler):
    """
    HuggingFace APIë¥¼ ìœ„í•œ ìµœì í™”ëœ ì½œë°± í•¸ë“¤ëŸ¬
    HuggingFace Inference APIë¥¼ í†µí•œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
    """
    
    def __init__(self, message_id: str, session_id: str, model_name: str = "huggingface"):
        self.tokens = []
        self.message_id = message_id
        self.session_id = session_id
        self.model_name = model_name
        self.current_content = ""
        self.stream_queue = asyncio.Queue()
        self.is_streaming = False
        self.tool_calls = []
        self.start_time = None
        self.token_count = 0
    
    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        """ìƒˆ í† í° ìƒì„± ì‹œ í˜¸ì¶œ - HuggingFace API ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”"""
        try:
            # HuggingFace API í† í° ì²˜ë¦¬
            token_str = self._process_huggingface_api_token(token)
            
            if token_str and token_str.strip():  # ë¹ˆ í† í° í•„í„°ë§
                self.tokens.append(token_str)
                self.current_content += token_str
                self.token_count += 1
                
                # ìŠ¤íŠ¸ë¦¬ë° íì— í† í° ì¶”ê°€
                await self.stream_queue.put({
                    "type": "content",
                    "content": token_str,
                    "message_id": self.message_id,
                    "session_id": self.session_id,
                    "model": self.model_name,
                    "token_count": self.token_count,
                    "timestamp": kr_time_now().isoformat()
                })
                
        except Exception as e:
            print(f"âŒ Error in HuggingFace API token processing: {e}")
            print(f"ğŸ” Token: {type(token)} - {repr(token)}")
            # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ì²˜ë¦¬ ê³„ì†
    
    def _process_huggingface_api_token(self, token: Any) -> str:
        """
        HuggingFace API ì‘ë‹µ í† í° ì²˜ë¦¬
        APIì—ì„œ ë°›ì€ ìŠ¤íŠ¸ë¦¬ë° í† í°ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        """
        if isinstance(token, str):
            # ê°€ì¥ ì¼ë°˜ì ì¸ ê²½ìš°: ë¬¸ìì—´ í† í°
            return token
        
        elif isinstance(token, dict):
            # API ì‘ë‹µì—ì„œ êµ¬ì¡°í™”ëœ ë°ì´í„°
            if 'token' in token:
                # {'token': {'text': '...'}} í˜•ì‹
                token_data = token['token']
                if isinstance(token_data, dict) and 'text' in token_data:
                    return token_data['text']
                return str(token_data)
            
            elif 'generated_text' in token:
                return token['generated_text']
            elif 'text' in token:
                return token['text']
            elif 'content' in token:
                return token['content']
            elif 'choices' in token and len(token['choices']) > 0:
                # OpenAI ìŠ¤íƒ€ì¼ ì‘ë‹µ
                choice = token['choices'][0]
                if 'delta' in choice and 'content' in choice['delta']:
                    return choice['delta']['content']
                elif 'text' in choice:
                    return choice['text']
            
            # ë‹¤ë¥¸ êµ¬ì¡°ì˜ ë”•ì…”ë„ˆë¦¬
            return str(token)
        
        elif isinstance(token, list) and len(token) > 0:
            # ë°°ì¹˜ ì‘ë‹µì˜ ì²« ë²ˆì§¸ í•­ëª© ì²˜ë¦¬
            return self._process_huggingface_api_token(token[0])
        
        elif token is None:
            return ""
        
        else:
            # ê¸°íƒ€ íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
            return str(token)
    
    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs) -> None:
        """LLM ì‹œì‘"""
        self.tokens = []
        self.current_content = ""
        self.is_streaming = True
        self.start_time = time.time()
        self.token_count = 0
        
        await self.stream_queue.put({
            "type": "start",
            "message_id": self.message_id,
            "session_id": self.session_id,
            "model": self.model_name,
            "timestamp": kr_time_now().isoformat()
        })
    
    async def on_llm_end(self, response: LLMResult, **kwargs) -> None:
        """LLM ì¢…ë£Œ - HuggingFace API ì‘ë‹µ ì™„ë£Œ ì²˜ë¦¬"""
        self.is_streaming = False
        end_time = time.time()
        duration = end_time - self.start_time if self.start_time else 0
        
        # API ì‘ë‹µì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
        usage_info = self._extract_usage_info(response) if response else {}
        
        await self.stream_queue.put({
            "type": "end",
            "message_id": self.message_id,
            "session_id": self.session_id,
            "model": self.model_name,
            "timestamp": kr_time_now().isoformat(),
            "final_content": self.current_content,
            "duration": duration,
            "token_count": self.token_count,
            "tokens_per_second": self.token_count / duration if duration > 0 else 0,
            "usage": usage_info
        })
    
    def _extract_usage_info(self, response: LLMResult) -> Dict[str, Any]:
        """API ì‘ë‹µì—ì„œ ì‚¬ìš©ëŸ‰ ì •ë³´ ì¶”ì¶œ"""
        usage_info = {}
        
        if hasattr(response, 'llm_output') and response.llm_output:
            llm_output = response.llm_output
            
            # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´
            if 'token_usage' in llm_output:
                usage_info['api_token_usage'] = llm_output['token_usage']
            
            # ëª¨ë¸ ì •ë³´
            if 'model_name' in llm_output:
                usage_info['api_model'] = llm_output['model_name']
        
        return usage_info
    
    async def on_llm_error(self, error: Exception, **kwargs) -> None:
        """LLM ì—ëŸ¬"""
        self.is_streaming = False
        
        await self.stream_queue.put({
            "type": "error",
            "message_id": self.message_id,
            "session_id": self.session_id,
            "model": self.model_name,
            "timestamp": kr_time_now().isoformat(),
            "error": str(error),
            "error_type": type(error).__name__
        })
    
    async def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs) -> None:
        """íˆ´ ì‹œì‘"""
        tool_name = serialized.get("name", "unknown")
        tool_start_time = time.time()
        
        tool_call = {
            "tool": tool_name,
            "input": input_str,
            "status": "started",
            "start_time": tool_start_time
        }
        self.tool_calls.append(tool_call)
        
        print(f"ğŸ”§ Tool '{tool_name}' started (HuggingFace: {self.model_name})")
        
        await self.stream_queue.put({
            "type": "tool_start",
            "tool_name": tool_name,
            "tool_input": input_str,
            "message_id": self.message_id,
            "session_id": self.session_id,
            "model": self.model_name,
            "timestamp": kr_time_now().isoformat()
        })
    
    async def on_tool_end(self, output: str, **kwargs) -> None:
        """íˆ´ ì¢…ë£Œ"""
        tool_end_time = time.time()
        
        if self.tool_calls:
            tool_call = self.tool_calls[-1]
            tool_call["status"] = "completed"
            tool_call["result"] = output[:200] + "..." if len(output) > 200 else output
            tool_call["end_time"] = tool_end_time
            
            if "start_time" in tool_call:
                tool_duration = tool_end_time - tool_call["start_time"]
                tool_call["duration"] = tool_duration
                print(f"ğŸ”§ Tool '{tool_call['tool']}' completed in {tool_duration:.3f}s")
        
        await self.stream_queue.put({
            "type": "tool_end",
            "tool_result": output[:200] + "..." if len(output) > 200 else output,
            "message_id": self.message_id,
            "session_id": self.session_id,
            "model": self.model_name,
            "timestamp": kr_time_now().isoformat()
        })
    
    async def on_agent_action(self, action, **kwargs) -> None:
        """ì—ì´ì „íŠ¸ ì•¡ì…˜"""
        await self.stream_queue.put({
            "type": "thinking",
            "thought": f"Using tool: {action.tool}",
            "message_id": self.message_id,
            "session_id": self.session_id,
            "model": self.model_name,
            "timestamp": kr_time_now().isoformat()
        })
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        duration = time.time() - self.start_time if self.start_time else 0
        return {
            "model": self.model_name,
            "token_count": self.token_count,
            "content_length": len(self.current_content),
            "tool_calls_count": len(self.tool_calls),
            "duration": duration,
            "tokens_per_second": self.token_count / duration if duration > 0 else 0,
            "is_streaming": self.is_streaming,
            "tool_calls": self.tool_calls
        }


def get_huggingface_callback_handler(
    message_id: str, 
    session_id: str, 
    model_name: str = "huggingface"
) -> HuggingFaceCallbackHandler:
    """
    HuggingFace ì½œë°± í•¸ë“¤ëŸ¬ íŒ©í† ë¦¬ í•¨ìˆ˜
    
    Args:
        message_id: ë©”ì‹œì§€ ID
        session_id: ì„¸ì…˜ ID  
        model_name: ëª¨ë¸ ì´ë¦„ (ë¡œê¹…ìš©)
        
    Returns:
        HuggingFaceCallbackHandler ì¸ìŠ¤í„´ìŠ¤
    """
    return HuggingFaceCallbackHandler(
        message_id=message_id,
        session_id=session_id, 
        model_name=model_name
    )