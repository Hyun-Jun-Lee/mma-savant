"""
ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
LangChain Agentì˜ ì¶œë ¥ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ê³  ì •ì œí•˜ëŠ” í•¨ìˆ˜ë“¤
"""
from typing import Dict, Any, List, Union, Optional
from datetime import datetime

from common.logging_config import get_logger
from common.utils import remove_timestamps_from_tool_result, kr_time_now

LOGGER = get_logger(__name__)


def extract_safe_text_content(content: Any) -> str:
    """
    LangChain Agent ì¶œë ¥ì—ì„œ ì•ˆì „í•˜ê²Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    Anthropic í† í° í˜•íƒœì™€ ê¸°íƒ€ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ í•„í„°ë§
    
    Args:
        content: ì¶”ì¶œí•  ì½˜í…ì¸  (str, dict, list ë“±)
        
    Returns:
        str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì½˜í…ì¸ 
    """
    if isinstance(content, str):
        return content
    
    elif isinstance(content, dict):
        # Anthropic í† í° í˜•íƒœ í•„í„°ë§
        if content.get('type') in ['tool_use', 'input_json_delta', 'tool_call', 'function_call']:
            LOGGER.debug(f"ğŸ”§ Filtered tool-related token: {content.get('type')}")
            return ""
        
        # íˆ´ ID í† í° í•„í„°ë§
        if 'id' in content and str(content.get('id', '')).startswith('toolu_'):
            LOGGER.debug(f"ğŸ”§ Filtered tool ID token: {content.get('id')}")
            return ""
        
        # í‘œì¤€ í…ìŠ¤íŠ¸ í•„ë“œë“¤ í™•ì¸ (ìš°ì„ ìˆœìœ„ ìˆœ)
        for text_field in ['content', 'text', 'message']:
            if text_field in content:
                text_value = content[text_field]
                if isinstance(text_value, str):
                    return text_value
                else:
                    # ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
                    return extract_safe_text_content(text_value)
        
        # ì•Œë ¤ì§„ êµ¬ì¡°í™”ëœ í˜•íƒœë“¤ ë¡œê¹…
        if 'type' in content and 'index' in content:
            LOGGER.debug(f"ğŸ” Skipped structured token: {content}")
            return ""
        
        # ê¸°íƒ€ ë”•ì…”ë„ˆë¦¬ëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
        LOGGER.debug(f"âš ï¸ Unknown dict structure converted to string: {content}")
        return str(content)
    
    elif isinstance(content, list):
        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê° í•­ëª©ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ì—¬ í•©ì¹¨
        texts = []
        for item in content:
            extracted = extract_safe_text_content(item)
            if extracted:
                texts.append(extracted)
        return ''.join(texts)
    
    else:
        # ê¸°íƒ€ íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
        return str(content) if content is not None else ""


def process_agent_chunk(chunk: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì—ì´ì „íŠ¸ ì²­í¬ ì²˜ë¦¬ ë° ì •ê·œí™”
    
    Args:
        chunk: Agent executorì—ì„œ ë°›ì€ ì²­í¬
        
    Returns:
        Dict: ì²˜ë¦¬ëœ ì²­í¬ ì •ë³´
    """
    processed_chunk = {
        "type": "unknown",
        "content": "",
        "metadata": {}
    }
    
    try:
        if isinstance(chunk, dict):
            # ì¶œë ¥ ì²­í¬ ì²˜ë¦¬
            if "output" in chunk:
                content = chunk["output"]
                extracted_text = extract_safe_text_content(content)
                processed_chunk.update({
                    "type": "content",
                    "content": extracted_text,
                    "metadata": {
                        "original_type": type(content).__name__,
                        "has_output": True
                    }
                })
            
            # ì¤‘ê°„ ë‹¨ê³„ ì²˜ë¦¬
            elif "intermediate_steps" in chunk:
                steps = chunk["intermediate_steps"]
                processed_chunk.update({
                    "type": "intermediate_steps",
                    "content": "",
                    "metadata": {
                        "steps_count": len(steps),
                        "steps": steps
                    }
                })
            
            # ê¸°íƒ€ ì²­í¬ íƒ€ì…
            else:
                processed_chunk.update({
                    "type": "raw_chunk",
                    "content": extract_safe_text_content(chunk),
                    "metadata": {
                        "keys": list(chunk.keys()) if isinstance(chunk, dict) else [],
                        "original_type": type(chunk).__name__
                    }
                })
        
        else:
            # ë¹„-ë”•ì…”ë„ˆë¦¬ ì²­í¬ ì²˜ë¦¬
            processed_chunk.update({
                "type": "raw_content",
                "content": extract_safe_text_content(chunk),
                "metadata": {
                    "original_type": type(chunk).__name__
                }
            })
    
    except Exception as e:
        LOGGER.error(f"âŒ Error processing chunk: {e}")
        processed_chunk.update({
            "type": "error",
            "content": "",
            "metadata": {
                "error": str(e),
                "original_chunk": str(chunk)[:100] + "..." if len(str(chunk)) > 100 else str(chunk)
            }
        })
    
    return processed_chunk


def extract_tool_results(intermediate_steps: List[Any]) -> List[Dict[str, Any]]:
    """
    ì¤‘ê°„ ë‹¨ê³„ì—ì„œ ë„êµ¬ ê²°ê³¼ ì¶”ì¶œ
    v1 í˜¸í™˜ì„±ì„ ìœ„í•´ ë‹¨ìˆœí™”ëœ ë¡œì§ ì‚¬ìš©
    
    Args:
        intermediate_steps: Agent executorì˜ intermediate_steps
        
    Returns:
        List[Dict]: ì¶”ì¶œëœ ë„êµ¬ ê²°ê³¼ë“¤
    """
    tool_results = []
    
    try:
        LOGGER.info(f"ğŸ”§ Found intermediate_steps: {len(intermediate_steps)} steps")
        
        for step in intermediate_steps:
            try:
                if len(step) >= 2:
                    action, observation = step
                    
                    # v1ê³¼ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬
                    tool_result = {
                        "tool": getattr(action, 'tool', 'unknown'),
                        "input": str(action.tool_input),
                        "result": str(remove_timestamps_from_tool_result(observation))
                    }
                    tool_results.append(tool_result)
                    
            except Exception as e:
                LOGGER.error(f"âŒ Error processing individual step: {e}")
                # ì—ëŸ¬ê°€ ìˆì–´ë„ ê³„ì† ì²˜ë¦¬
                continue
        
        LOGGER.info(f"âœ… Extracted {len(tool_results)} tool results")
        return tool_results
        
    except Exception as e:
        LOGGER.error(f"âŒ Error extracting tool results: {e}")
        return []


def clean_response_content(content: Any) -> str:
    """
    ì‘ë‹µ ì½˜í…ì¸  ì •ë¦¬ ë° ê²€ì¦
    
    Args:
        content: ì •ë¦¬í•  ì½˜í…ì¸ 
        
    Returns:
        str: ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ì½˜í…ì¸ 
    """
    try:
        if isinstance(content, str):
            cleaned = content.strip()
        else:
            LOGGER.warning(f"âš ï¸ Non-string response_content: {type(content)} - {content}")
            cleaned = extract_safe_text_content(content)
        
        # ë¹ˆ ì½˜í…ì¸  í™•ì¸
        if not cleaned:
            LOGGER.warning("âš ï¸ Empty response content after cleaning")
            return ""
        
        # ì½˜í…ì¸  ê¸¸ì´ ë¡œê¹…
        LOGGER.debug(f"ğŸ“ Cleaned content length: {len(cleaned)} characters")
        
        return cleaned
        
    except Exception as e:
        LOGGER.error(f"âŒ Error cleaning response content: {e}")
        return ""


def create_final_result(
    content: str,
    tool_results: List[Dict[str, Any]],
    message_id: str,
    conversation_id : int,
    execution_time: float,
    user_id: Optional[int] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    ìµœì¢… ê²°ê³¼ ê°ì²´ ìƒì„±
    
    Args:
        content: ìµœì¢… ì‘ë‹µ ì½˜í…ì¸ 
        tool_results: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë“¤
        message_id: ë©”ì‹œì§€ ID
        conversation_id: ì„¸ì…˜ ID
        execution_time: ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
        **kwargs: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        
    Returns:
        Dict: ìµœì¢… ê²°ê³¼ ê°ì²´
    """
    result = {
        "type": "final_result",
        "content": content,
        "tool_results": tool_results,
        "message_id": message_id,
        "conversation_id": conversation_id,
        "timestamp": kr_time_now().isoformat(),
        "execution_time": execution_time,
        "metadata": {
            "content_length": len(content),
            "tools_used_count": len(tool_results),
            "has_tools": len(tool_results) > 0,
            **kwargs
        }
    }
    
    # user_idê°€ ìˆìœ¼ë©´ í¬í•¨
    if user_id is not None:
        result["user_id"] = user_id
    
    return result


def create_error_response(
    error: Exception,
    message_id: str,
    conversation_id : int,
    context: Optional[Dict[str, Any]] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    ì—ëŸ¬ ì‘ë‹µ ê°ì²´ ìƒì„±
    
    Args:
        error: ë°œìƒí•œ ì—ëŸ¬
        message_id: ë©”ì‹œì§€ ID
        conversation_id: ì„¸ì…˜ ID  
        context: ì—ëŸ¬ ë°œìƒ ì»¨í…ìŠ¤íŠ¸
        **kwargs: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        
    Returns:
        Dict: ì—ëŸ¬ ì‘ë‹µ ê°ì²´
    """
    error_message = str(error)
    
    # íŠ¹ë³„í•œ ì—ëŸ¬ íƒ€ì… ì²˜ë¦¬
    if "rate_limit_error" in error_message or "429" in error_message:
        LOGGER.warning("ğŸš« Rate limit exceeded")
        user_friendly_message = "API í˜¸ì¶œ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    else:
        user_friendly_message = error_message
    
    return {
        "type": "error",
        "error": user_friendly_message,
        "message_id": message_id,
        "conversation_id": conversation_id,
        "timestamp": kr_time_now().isoformat(),
        "metadata": {
            "error_type": type(error).__name__,
            "original_error": error_message,
            "context": context or {},
            **kwargs
        }
    }


def create_streaming_chunk(
    chunk_type: str,
    content: str,
    message_id: str,
    conversation_id : int,
    **metadata
) -> Dict[str, Any]:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ê°ì²´ ìƒì„±
    
    Args:
        chunk_type: ì²­í¬ íƒ€ì… (content, tool_start, tool_end ë“±)
        content: ì²­í¬ ì½˜í…ì¸ 
        message_id: ë©”ì‹œì§€ ID
        conversation_id: ì„¸ì…˜ ID
        **metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
        
    Returns:
        Dict: ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ê°ì²´
    """
    return {
        "type": chunk_type,
        "content": content,
        "message_id": message_id,
        "conversation_id": conversation_id,
        "timestamp": kr_time_now().isoformat(),
        "metadata": metadata
    }


def validate_streaming_chunk(chunk: Dict[str, Any]) -> bool:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ìœ íš¨ì„± ê²€ì‚¬
    
    Args:
        chunk: ê²€ì‚¬í•  ì²­í¬
        
    Returns:
        bool: ìœ íš¨í•œ ì²­í¬ì¸ì§€ ì—¬ë¶€
    """
    required_fields = ["type", "message_id", "conversation_id", "timestamp"]
    
    if not isinstance(chunk, dict):
        return False
    
    for field in required_fields:
        if field not in chunk:
            LOGGER.warning(f"âš ï¸ Missing required field in chunk: {field}")
            return False
    
    # íƒ€ì…ë³„ íŠ¹ë³„ ê²€ì¦
    chunk_type = chunk.get("type")
    if chunk_type == "content" and not isinstance(chunk.get("content"), str):
        LOGGER.warning("âš ï¸ Content chunk must have string content")
        return False
    
    return True


def get_chunk_statistics(chunks: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ì²­í¬ í†µê³„ ì •ë³´ ìƒì„±
    
    Args:
        chunks: ë¶„ì„í•  ì²­í¬ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        Dict: í†µê³„ ì •ë³´
    """
    if not chunks:
        return {"total_chunks": 0}
    
    stats = {
        "total_chunks": len(chunks),
        "chunk_types": {},
        "total_content_length": 0,
        "average_chunk_size": 0,
        "first_chunk_time": None,
        "last_chunk_time": None
    }
    
    content_lengths = []
    timestamps = []
    
    for chunk in chunks:
        # íƒ€ì…ë³„ ì¹´ìš´íŠ¸
        chunk_type = chunk.get("type", "unknown")
        stats["chunk_types"][chunk_type] = stats["chunk_types"].get(chunk_type, 0) + 1
        
        # ì½˜í…ì¸  ê¸¸ì´
        content = chunk.get("content", "")
        if isinstance(content, str):
            length = len(content)
            content_lengths.append(length)
            stats["total_content_length"] += length
        
        # íƒ€ì„ìŠ¤íƒ¬í”„
        timestamp = chunk.get("timestamp")
        if timestamp:
            timestamps.append(timestamp)
    
    # í†µê³„ ê³„ì‚°
    if content_lengths:
        stats["average_chunk_size"] = sum(content_lengths) / len(content_lengths)
    
    if timestamps:
        stats["first_chunk_time"] = min(timestamps)
        stats["last_chunk_time"] = max(timestamps)
    
    return stats


if __name__ == "__main__":
    """í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹…ìš©"""
    print("ğŸ”„ Stream Processor Test")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì½˜í…ì¸ ë“¤
    test_contents = [
        "Simple string content",
        {"content": "Dict with content field"},
        {"type": "tool_use", "id": "toolu_123"},  # í•„í„°ë§ë˜ì–´ì•¼ í•¨
        {"text": "Dict with text field"},
        ["List", "of", "strings"],
        None,
        42
    ]
    
    print("\nğŸ“ Text Extraction Test:")
    for i, content in enumerate(test_contents):
        extracted = extract_safe_text_content(content)
        print(f"  {i+1}. {type(content).__name__}: '{extracted}'")
    
    # ê°€ìƒ ë„êµ¬ ê²°ê³¼ í…ŒìŠ¤íŠ¸
    print("\nğŸ”§ Tool Results Test:")
    class MockAction:
        def __init__(self, tool, tool_input):
            self.tool = tool
            self.tool_input = tool_input
    
    mock_steps = [
        (MockAction("search", "MMA fighters"), "Found 10 fighters"),
        (MockAction("analyze", {"fighter": "Jon Jones"}), "Analysis complete")
    ]
    
    tool_results = extract_tool_results(mock_steps)
    for result in tool_results:
        print(f"  Tool: {result['tool']}, Input: {result['input']}")
    
    print(f"\nâœ… Stream Processor module loaded successfully")