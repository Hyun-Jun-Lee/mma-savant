from typing import Callable, Dict, Any, Optional

from langchain_huggingface import HuggingFacePipeline
from config import Config


def get_huggingface_llm(
    callback_handler: Callable,
    model_name: str = "microsoft/DialoGPT-medium",
    temperature: float = 0.7,
    max_tokens: int = 4000,
    device: int = -1,  # -1 for CPU, 0+ for GPU
    task: str = "text-generation",
    **kwargs
) -> HuggingFacePipeline:
    """
    HuggingFace LLM provider - Î™®Îì† Ï£ºÏöî LLM Î™®Îç∏Îì§ÏùÑ ÏßÄÏõê
    
    Ï£ºÏöî Î™®Îç∏ ÏòàÏãú:
    - "microsoft/DialoGPT-medium": ÎåÄÌôîÌòï Î™®Îç∏
    - "meta-llama/Llama-2-7b-chat-hf": MetaÏùò Llama Î™®Îç∏
    - "google/flan-t5-large": GoogleÏùò T5 Î™®Îç∏
    - "openai-gpt": OpenAI Ïä§ÌÉÄÏùº Î™®Îç∏
    - "EleutherAI/gpt-neo-2.7B": EleutherAI GPT
    
    Args:
        callback_handler: Ïä§Ìä∏Î¶¨Î∞çÏùÑ ÏúÑÌïú ÏΩúÎ∞± Ìï∏Îì§Îü¨
        model_name: HuggingFace HubÏùò Î™®Îç∏ Ïù¥Î¶Ñ
        temperature: ÏÉùÏÑ± Ïò®ÎèÑ (0.0-1.0)
        max_tokens: ÏµúÎåÄ ÌÜ†ÌÅ∞ Ïàò
        device: Ï∂îÎ°† ÎîîÎ∞îÏù¥Ïä§ (-1=CPU, 0+=GPU)
        task: HuggingFace ÌååÏù¥ÌîÑÎùºÏù∏ ÌÉúÏä§ÌÅ¨
        **kwargs: Ï∂îÍ∞Ä Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞
        
    Returns:
        ÏÑ§Ï†ïÎêú HuggingFacePipeline Ïù∏Ïä§ÌÑ¥Ïä§
    """
    from transformers import pipeline
    
    # Î™®Îç∏Î≥Ñ Í∏∞Î≥∏ ÏÑ§Ï†ï
    model_configs = {
        "microsoft/DialoGPT-medium": {
            "task": "text-generation",
            "pad_token_id": 50256
        },
        "meta-llama/Llama-2-7b-chat-hf": {
            "task": "text-generation",
            "do_sample": True,
            "top_p": 0.9
        },
        "google/flan-t5-large": {
            "task": "text2text-generation",
            "do_sample": True
        },
        "EleutherAI/gpt-neo-2.7B": {
            "task": "text-generation",
            "do_sample": True
        }
    }
    
    # Î™®Îç∏Î≥Ñ ÏÑ§Ï†ï Ï†ÅÏö©
    config = model_configs.get(model_name, {"task": task})
    final_task = config.get("task", task)
    
    # Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï
    model_kwargs = {
        "temperature": temperature,
        "max_length": max_tokens,
        **config,
        **kwargs
    }
    
    # task Ï†úÍ±∞ (pipeline ÌååÎùºÎØ∏ÌÑ∞Ïù¥ÎØÄÎ°ú)
    model_kwargs.pop("task", None)
    
    try:
        # HuggingFace ÌååÏù¥ÌîÑÎùºÏù∏ ÏÉùÏÑ±
        hf_pipeline = pipeline(
            final_task,
            model=model_name,
            device=device,
            model_kwargs=model_kwargs
        )
        
        # LangChain ÎûòÌçº ÏÉùÏÑ±
        return HuggingFacePipeline(
            pipeline=hf_pipeline,
            callbacks=[callback_handler]
        )
        
    except Exception as e:
        print(f"‚ùå Error loading HuggingFace model '{model_name}': {e}")
        print(f"üí° Falling back to default model...")
        
        # Í∏∞Î≥∏ Î™®Îç∏Î°ú Ìè¥Î∞±
        fallback_pipeline = pipeline(
            "text-generation",
            model="microsoft/DialoGPT-medium",
            device=device,
            model_kwargs={
                "temperature": temperature,
                "max_length": max_tokens,
                "pad_token_id": 50256
            }
        )
        
        return HuggingFacePipeline(
            pipeline=fallback_pipeline,
            callbacks=[callback_handler]
        )


def get_chat_model_llm(
    callback_handler: Callable,
    model_type: str = "llama",
    size: str = "7b",
    **kwargs
) -> HuggingFacePipeline:
    """
    Ï±ÑÌåÖ ÌäπÌôî Î™®Îç∏Îì§ÏùÑ ÏúÑÌïú Ìé∏Ïùò Ìï®Ïàò
    
    Args:
        callback_handler: ÏΩúÎ∞± Ìï∏Îì§Îü¨
        model_type: Î™®Îç∏ ÌÉÄÏûÖ (llama, flan-t5, dialogpt)
        size: Î™®Îç∏ ÌÅ¨Í∏∞ (7b, 13b, large, medium Îì±)
        **kwargs: Ï∂îÍ∞Ä ÌååÎùºÎØ∏ÌÑ∞
    """
    
    model_map = {
        "llama": {
            "7b": "meta-llama/Llama-2-7b-chat-hf",
            "13b": "meta-llama/Llama-2-13b-chat-hf"
        },
        "flan-t5": {
            "small": "google/flan-t5-small",
            "base": "google/flan-t5-base", 
            "large": "google/flan-t5-large",
            "xl": "google/flan-t5-xl"
        },
        "dialogpt": {
            "small": "microsoft/DialoGPT-small",
            "medium": "microsoft/DialoGPT-medium",
            "large": "microsoft/DialoGPT-large"
        },
        "gpt-neo": {
            "1.3b": "EleutherAI/gpt-neo-1.3B",
            "2.7b": "EleutherAI/gpt-neo-2.7B"
        }
    }
    
    model_name = model_map.get(model_type, {}).get(size)
    if not model_name:
        print(f"‚ùå Unknown model combination: {model_type}/{size}")
        model_name = "microsoft/DialoGPT-medium"  # Í∏∞Î≥∏Í∞í
    
    return get_huggingface_llm(
        callback_handler=callback_handler,
        model_name=model_name,
        **kwargs
    )


def list_popular_models() -> Dict[str, Dict[str, str]]:
    """Ïù∏Í∏∞ ÏûàÎäî HuggingFace Î™®Îç∏Îì§ Î™©Î°ù Î∞òÌôò"""
    return {
        "chat_models": {
            "llama2_7b": "meta-llama/Llama-2-7b-chat-hf",
            "llama2_13b": "meta-llama/Llama-2-13b-chat-hf", 
            "dialogpt_medium": "microsoft/DialoGPT-medium",
            "dialogpt_large": "microsoft/DialoGPT-large"
        },
        "instruction_models": {
            "flan_t5_large": "google/flan-t5-large",
            "flan_t5_xl": "google/flan-t5-xl",
            "alpaca": "tatsu-lab/alpaca-7b-wdiff"
        },
        "general_models": {
            "gpt_neo_2.7b": "EleutherAI/gpt-neo-2.7B",
            "gpt_j_6b": "EleutherAI/gpt-j-6B",
            "opt_2.7b": "facebook/opt-2.7b"
        },
        "lightweight_models": {
            "distilgpt2": "distilgpt2",
            "gpt2_medium": "gpt2-medium",
            "dialogpt_small": "microsoft/DialoGPT-small"
        }
    }