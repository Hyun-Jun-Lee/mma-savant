from typing import Callable, Dict, Any, Optional

from langchain_huggingface import HuggingFaceEndpoint
from config import Config


def get_huggingface_llm(
    callback_handler: Callable,
    model_name: str = None,
    temperature: float = None,
    max_tokens: int = None,
    huggingface_api_token: str = None,
    **kwargs
) -> HuggingFaceEndpoint:
    """
    HuggingFace API LLM provider - HuggingFace Inference APIë¥¼ í†µí•´ ëª¨ë¸ ì‚¬ìš©
    
    ì£¼ìš” ëª¨ë¸ ì˜ˆì‹œ:
    - "microsoft/DialoGPT-medium": ëŒ€í™”í˜• ëª¨ë¸
    - "meta-llama/Llama-2-7b-chat-hf": Metaì˜ Llama ëª¨ë¸
    - "google/flan-t5-large": Googleì˜ T5 ëª¨ë¸
    - "mistralai/Mistral-7B-Instruct-v0.1": Mistral ëª¨ë¸
    - "HuggingFaceH4/zephyr-7b-beta": Zephyr ëª¨ë¸
    
    Args:
        callback_handler: ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì½œë°± í•¸ë“¤ëŸ¬
        model_name: HuggingFace Hubì˜ ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸ê°’: Configì—ì„œ ê°€ì ¸ì˜´)
        temperature: ìƒì„± ì˜¨ë„ (0.0-1.0)
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        huggingface_api_token: HuggingFace API í† í°
        **kwargs: ì¶”ê°€ ëª¨ë¸ íŒŒë¼ë¯¸í„°
        
    Returns:
        ì„¤ì •ëœ HuggingFaceEndpoint ì¸ìŠ¤í„´ìŠ¤
    """
    # ê¸°ë³¸ê°’ ì„¤ì •
    model_name = model_name or Config.HUGGINGFACE_MODEL_NAME
    temperature = temperature if temperature is not None else Config.HUGGINGFACE_TEMPERATURE
    max_tokens = max_tokens or Config.HUGGINGFACE_MAX_TOKENS
    api_token = huggingface_api_token or Config.HUGGINGFACE_API_TOKEN
    
    if not api_token:
        raise ValueError(
            "HuggingFace API token is required. "
            "Set HUGGINGFACE_API_TOKEN environment variable or pass huggingface_api_token parameter."
        )

    # ìµœì¢… ëª¨ë¸ íŒŒë¼ë¯¸í„°
    model_kwargs = {
        **kwargs
    }
    
    try:
        # HuggingFace Inference API ì—”ë“œí¬ì¸íŠ¸ ìƒì„±
        return HuggingFaceEndpoint(
            endpoint_url=f"https://api-inference.huggingface.co/models/{model_name}",
            huggingfacehub_api_token=api_token,
            model_kwargs=model_kwargs,
            callbacks=[callback_handler],
            streaming=True  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        )
        
    except Exception as e:
        print(f"âŒ Error connecting to HuggingFace API for model '{model_name}': {e}")
        print(f"ğŸ’¡ Falling back to default model...")
        
        # ê¸°ë³¸ ëª¨ë¸ë¡œ í´ë°±
        return HuggingFaceEndpoint(
            endpoint_url="https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium",
            huggingfacehub_api_token=api_token,
            model_kwargs=model_kwargs,
            callbacks=[callback_handler],
            streaming=True
        )


def get_chat_model_llm(
    callback_handler: Callable,
    model_name: str,
    huggingface_api_token: str = None,
    **kwargs
) -> HuggingFaceEndpoint:
    """
    ì±„íŒ… íŠ¹í™” ëª¨ë¸ë“¤ì„ ìœ„í•œ í¸ì˜ í•¨ìˆ˜
    
    Args:
        callback_handler: ì½œë°± í•¸ë“¤ëŸ¬
        model_name: ëª¨ë¸ ì´ë¦„
        **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
    """
    
    return get_huggingface_llm(
        callback_handler=callback_handler,
        model_name=model_name,
        huggingface_api_token=huggingface_api_token,
        **kwargs
    )