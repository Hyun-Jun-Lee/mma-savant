import json
import asyncio
from typing import Callable, Dict, Any, AsyncGenerator, Optional, List

from openai import AsyncOpenAI
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage
from langchain_core.outputs import LLMResult, Generation, ChatGeneration
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.prompt_values import ChatPromptValue, StringPromptValue
from pydantic import Field

from config import Config
from common.logging_config import get_logger
from common.utils import kr_time_now

LOGGER = get_logger(__name__)


class OpenRouterLLM(BaseChatModel):
    """OpenRouterë¥¼ ì‚¬ìš©í•˜ëŠ” LangChain í˜¸í™˜ LLM í´ë˜ìŠ¤ (ê³µì‹ ë¬¸ì„œ ë°©ì‹)"""
    
    # Pydantic í•„ë“œ ì •ì˜
    model_name: str = Field(...)
    api_key: str = Field(...)
    base_url: str = Field(default="https://openrouter.ai/api/v1")
    temperature: float = Field(default=0.7)
    max_tokens: int = Field(default=4000)
    callback_handler: Optional[Any] = Field(default=None)  # Any íƒ€ì…ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì½œë°± í•¸ë“¤ëŸ¬ ê°ì²´ í—ˆìš©
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸ëŠ” ëŸ°íƒ€ì„ì— ìƒì„±
    _client: Optional[AsyncOpenAI] = None
    
    class Config:
        arbitrary_types_allowed = True
        validate_assignment = False  # í• ë‹¹ ì‹œ ê²€ì¦ ë¹„í™œì„±í™”
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ê³µì‹ ë¬¸ì„œ ë°©ì‹)
        self._client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
    
    @property
    def client(self) -> AsyncOpenAI:
        if self._client is None:
            self._client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )
        return self._client
    
    @property
    def _llm_type(self) -> str:
        return "openrouter"
    
    def _generate(
        self,
        messages: List[List[BaseMessage]],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs
    ) -> LLMResult:
        """ë™ê¸° ìƒì„± ë©”ì„œë“œ (LangChain ìš”êµ¬ì‚¬í•­)"""
        # ë™ê¸° ë©”ì„œë“œëŠ” êµ¬í˜„í•˜ì§€ ì•ŠìŒ
        raise NotImplementedError("Use agenerate for async operations")
    
    async def _agenerate(
        self,
        messages: List[List[BaseMessage]],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs
    ) -> LLMResult:
        """LangChain í˜¸í™˜ ë¹„ë™ê¸° ìƒì„± ë©”ì„œë“œ"""
        
        # ëª¨ë¸ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        model_params = get_model_specific_params(self.model_name)
        
        # ì²« ë²ˆì§¸ ë©”ì‹œì§€ ì‹œí€€ìŠ¤ ì‚¬ìš©
        message_list = messages[0] if messages else []
        
        # LangChain ë©”ì‹œì§€ë¥¼ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        openai_messages = []
        for msg in message_list:
            if isinstance(msg, HumanMessage):
                openai_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                openai_messages.append({"role": "assistant", "content": msg.content})
            else:
                # ê¸°íƒ€ ë©”ì‹œì§€ íƒ€ì…ì€ systemìœ¼ë¡œ ì²˜ë¦¬
                openai_messages.append({"role": "system", "content": msg.content})
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° êµ¬ì„± (ê³µì‹ ë¬¸ì„œ ë°©ì‹)
        request_params = {
            "model": self.model_name,
            "messages": openai_messages,
            "temperature": model_params.get("temperature", self.temperature),
            "max_tokens": model_params.get("max_tokens", self.max_tokens),
            "extra_headers": {
                "HTTP-Referer": "https://mma-savant.com",
                "X-Title": "MMA Savant"
            },
            "extra_body": {}
        }
        
        # ë°”ì¸ë”©ëœ ë„êµ¬ë“¤ì´ ìˆëŠ” ê²½ìš° tools íŒŒë¼ë¯¸í„° ì¶”ê°€
        bound_tools = getattr(self, '_bound_tools', [])
        if bound_tools:
            # LangChain ë„êµ¬ë¥¼ OpenAI function calling í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            openai_tools = []
            for tool in bound_tools:
                if hasattr(tool, 'name') and hasattr(tool, 'description'):
                    tool_def = {
                        "type": "function",
                        "function": {
                            "name": tool.name,
                            "description": tool.description,
                        }
                    }
                    # args_schemaê°€ ìˆëŠ” ê²½ìš° íŒŒë¼ë¯¸í„° ìŠ¤í‚¤ë§ˆ ì¶”ê°€
                    if hasattr(tool, 'args_schema') and tool.args_schema:
                        try:
                            tool_def["function"]["parameters"] = tool.args_schema.model_json_schema()
                        except:
                            pass  # ìŠ¤í‚¤ë§ˆ ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
                    openai_tools.append(tool_def)
            
            if openai_tools:
                request_params["tools"] = openai_tools
                request_params["tool_choice"] = "auto"
        
        # ëª¨ë¸ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„°ë¥¼ extra_bodyì— ì¶”ê°€
        for key, value in model_params.items():
            if key not in ["temperature", "max_tokens"]:
                request_params["extra_body"][key] = value
        
        LOGGER.info(f"ğŸš€ OpenRouter API call with tools: {len(bound_tools)} tools, extra_body: {request_params['extra_body']}")
        
        try:
            # API í˜¸ì¶œ (ê³µì‹ ë¬¸ì„œ ë°©ì‹)
            response = await self.client.chat.completions.create(**request_params)
            
            # ì‘ë‹µ ì²˜ë¦¬
            choice = response.choices[0]
            message = choice.message
            
            # ë„êµ¬ í˜¸ì¶œì´ ìˆëŠ” ê²½ìš°
            if hasattr(message, 'tool_calls') and message.tool_calls:
                # ë„êµ¬ í˜¸ì¶œ ì •ë³´ë¥¼ í¬í•¨í•œ ChatGeneration ìƒì„±
                ai_message = AIMessage(
                    content=message.content or "",
                    additional_kwargs={
                        "tool_calls": [
                            {
                                "id": tc.id,
                                "function": {
                                    "name": tc.function.name,
                                    "arguments": tc.function.arguments
                                }
                            } for tc in message.tool_calls
                        ]
                    }
                )
                generation = ChatGeneration(message=ai_message)
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µë„ ChatGenerationìœ¼ë¡œ ìƒì„±
                ai_message = AIMessage(content=message.content or "")
                generation = ChatGeneration(message=ai_message)
            
            return LLMResult(generations=[[generation]])
            
        except Exception as e:
            LOGGER.error(f"âŒ OpenRouter API call failed: {e}")
            raise
    
    # LangChain í•„ìˆ˜ ì¶”ìƒ ë©”ì„œë“œë“¤ êµ¬í˜„
    def generate_prompt(self, prompts, stop=None, callbacks=None, **kwargs):
        """ë™ê¸° í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        raise NotImplementedError("Use agenerate_prompt for async operations")
    
    async def agenerate_prompt(self, prompts, stop=None, callbacks=None, **kwargs):
        """ë¹„ë™ê¸° í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompt_strings = [p.to_string() for p in prompts]
        return await self._agenerate(prompt_strings, stop, callbacks, **kwargs)
    
    def predict(self, text: str, **kwargs) -> str:
        """ë™ê¸° ì˜ˆì¸¡"""
        raise NotImplementedError("Use apredict for async operations")
    
    async def apredict(self, text: str, **kwargs) -> str:
        """ë¹„ë™ê¸° ì˜ˆì¸¡"""
        result = await self._agenerate([text], **kwargs)
        return result.generations[0][0].text
    
    def predict_messages(self, messages, **kwargs):
        """ë™ê¸° ë©”ì‹œì§€ ì˜ˆì¸¡"""
        raise NotImplementedError("Use apredict_messages for async operations")
    
    async def apredict_messages(self, messages, **kwargs):
        """ë¹„ë™ê¸° ë©”ì‹œì§€ ì˜ˆì¸¡"""
        # ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
        prompt = messages[-1].content if messages else ""
        return await self.apredict(prompt, **kwargs)
    
    def invoke(self, input_data, config=None, **kwargs):
        """ë™ê¸° í˜¸ì¶œ"""
        raise NotImplementedError("Use ainvoke for async operations")
    
    async def ainvoke(self, input_data, config=None, **kwargs):
        """ë¹„ë™ê¸° í˜¸ì¶œ"""
        if isinstance(input_data, str):
            return await self.apredict(input_data, **kwargs)
        elif isinstance(input_data, list):
            return await self.apredict_messages(input_data, **kwargs)
        elif isinstance(input_data, ChatPromptValue):
            # ChatPromptValueë¥¼ ë©”ì‹œì§€ë¡œ ë³€í™˜
            messages = input_data.to_messages()
            return await self._agenerate_from_messages(messages, **kwargs)
        elif isinstance(input_data, StringPromptValue):
            # StringPromptValueë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            return await self.apredict(input_data.text, **kwargs)
        else:
            raise ValueError(f"Unsupported input type: {type(input_data)}")
    
    async def _agenerate_from_messages(self, messages: List[BaseMessage], **kwargs) -> str:
        """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ì‘ë‹µ ìƒì„±"""
        # ëª¨ë¸ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        model_params = get_model_specific_params(self.model_name)
        
        # LangChain ë©”ì‹œì§€ë¥¼ OpenAI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        openai_messages = []
        for msg in messages:
            if isinstance(msg, HumanMessage):
                openai_messages.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                openai_messages.append({"role": "assistant", "content": msg.content})
            else:
                # ê¸°íƒ€ ë©”ì‹œì§€ íƒ€ì…ì€ systemìœ¼ë¡œ ì²˜ë¦¬
                openai_messages.append({"role": "system", "content": msg.content})
        
        # ìš”ì²­ íŒŒë¼ë¯¸í„° êµ¬ì„±
        request_params = {
            "model": self.model_name,
            "messages": openai_messages,
            "temperature": model_params.get("temperature", self.temperature),
            "max_tokens": model_params.get("max_tokens", self.max_tokens),
            "extra_headers": {
                "HTTP-Referer": "https://mma-savant.com",
                "X-Title": "MMA Savant"
            },
            "extra_body": {}
        }
        
        # ë°”ì¸ë”©ëœ ë„êµ¬ë“¤ì´ ìˆëŠ” ê²½ìš° tools íŒŒë¼ë¯¸í„° ì¶”ê°€
        bound_tools = getattr(self, '_bound_tools', [])
        if bound_tools:
            openai_tools = []
            for tool in bound_tools:
                if hasattr(tool, 'name') and hasattr(tool, 'description'):
                    tool_def = {
                        "type": "function",
                        "function": {
                            "name": tool.name,
                            "description": tool.description,
                        }
                    }
                    if hasattr(tool, 'args_schema') and tool.args_schema:
                        try:
                            tool_def["function"]["parameters"] = tool.args_schema.model_json_schema()
                        except:
                            pass
                    openai_tools.append(tool_def)
            
            if openai_tools:
                request_params["tools"] = openai_tools
                request_params["tool_choice"] = "auto"
        
        # ëª¨ë¸ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„°ë¥¼ extra_bodyì— ì¶”ê°€
        for key, value in model_params.items():
            if key not in ["temperature", "max_tokens"]:
                request_params["extra_body"][key] = value
        
        try:
            # API í˜¸ì¶œ
            response = await self.client.chat.completions.create(**request_params)
            
            # ì‘ë‹µ ì²˜ë¦¬
            choice = response.choices[0]
            message = choice.message
            
            return message.content or ""
            
        except Exception as e:
            LOGGER.error(f"âŒ OpenRouter API call failed: {e}")
            raise
    
    def bind_tools(self, tools, **kwargs):
        """ë„êµ¬ ë°”ì¸ë”© (LangChain tool calling agent ìš”êµ¬ì‚¬í•­)"""
        # ë„êµ¬ ì •ë³´ë¥¼ ì €ì¥í•˜ê³  ìê¸° ìì‹ ì„ ë°˜í™˜ (ì²´ì´ë‹ íŒ¨í„´)
        bound_llm = self.__class__(
            model_name=self.model_name,
            api_key=self.api_key,
            base_url=self.base_url,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            callback_handler=self.callback_handler
        )
        # ë„êµ¬ ì •ë³´ ì €ì¥
        bound_llm._bound_tools = tools
        bound_llm._bind_kwargs = kwargs
        return bound_llm
    
    def get_bound_tools(self):
        """ë°”ì¸ë”©ëœ ë„êµ¬ë“¤ ë°˜í™˜"""
        return getattr(self, '_bound_tools', [])


def get_model_specific_params(model_name: str) -> Dict[str, Any]:
    """
    ëª¨ë¸ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„° ë°˜í™˜
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        
    Returns:
        Dict: ëª¨ë¸ì— íŠ¹í™”ëœ íŒŒë¼ë¯¸í„°ë“¤
    """
    model_params = {}
    
    # Llama-4-scout ëª¨ë¸ - extra_bodyë¡œ íŠ¹í™” íŒŒë¼ë¯¸í„° ì „ë‹¬
    if "llama-4-scout" in model_name.lower():
        model_params.update({
            "temperature": 0.7,
            "top_p": 0.9,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "min_rounds": 1,
            "max_rounds": 10
        })
    
    # Llama-3 ê³„ì—´ ëª¨ë¸ë“¤
    elif "llama-3" in model_name.lower():
        model_params.update({
            "top_p": 0.9,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        })
    
    # DeepSeek ëª¨ë¸ë“¤
    elif "deepseek" in model_name.lower():
        model_params.update({
            "top_p": 0.95,
            "temperature": 0.6
        })
    
    # GPT-4 ê³„ì—´ ëª¨ë¸ë“¤
    elif "gpt-4" in model_name.lower():
        model_params.update({
            "top_p": 1.0,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        })
    
    # Claude ê³„ì—´ ëª¨ë¸ë“¤
    elif "claude" in model_name.lower():
        model_params.update({
            "top_p": 0.9,
            "temperature": 0.7
        })
    
    # Mixtral ëª¨ë¸ë“¤
    elif "mixtral" in model_name.lower():
        model_params.update({
            "top_p": 0.9,
            "temperature": 0.7
        })
    
    # Gemini ëª¨ë¸ë“¤
    elif "gemini" in model_name.lower():
        model_params.update({
            "top_p": 0.95,
            "temperature": 0.7
        })
    
    return model_params


def get_model_info(model_name: str) -> Dict[str, Any]:
    """
    ëª¨ë¸ ì •ë³´ ë° ê¶Œì¥ ì„¤ì • ë°˜í™˜
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        
    Returns:
        Dict: ëª¨ë¸ ì •ë³´
    """
    if "llama-4-scout" in model_name.lower():
        return {
            "provider": "Meta",
            "type": "chat",
            "context_length": 8192,
            "description": "Metaì˜ ìµœì‹  Llama-4 ìŠ¤ì¹´ìš°íŠ¸ ëª¨ë¸",
            "strengths": ["reasoning", "code", "math"],
            "recommended_temp": 0.7
        }
    
    elif "deepseek" in model_name.lower():
        return {
            "provider": "DeepSeek",
            "type": "chat",
            "context_length": 32768,
            "description": "DeepSeekì˜ ê³ ì„±ëŠ¥ ì¶”ë¡  ëª¨ë¸",
            "strengths": ["reasoning", "code", "analysis"],
            "recommended_temp": 0.6
        }
    
    elif "gpt-4" in model_name.lower():
        return {
            "provider": "OpenAI",
            "type": "chat",
            "context_length": 128000 if "turbo" in model_name.lower() else 8192,
            "description": "OpenAIì˜ ê³ ì„±ëŠ¥ GPT-4 ëª¨ë¸",
            "strengths": ["general", "creative", "analysis"],
            "recommended_temp": 0.7
        }
    
    else:
        return {
            "provider": "Unknown",
            "type": "chat", 
            "context_length": 4096,
            "description": "ì¼ë°˜ ì±„íŒ… ëª¨ë¸",
            "strengths": ["general"],
            "recommended_temp": 0.7
        }


def get_openrouter_llm(
    callback_handler: Callable,
    model_name: str = Config.OPENROUTER_MODEL_NAME,
    api_key: str = Config.OPENROUTER_API_KEY,
    temperature: float = 0.7,
    max_tokens: int = None,
    **kwargs
):
    """
    OpenRouter LLM ìƒì„± (ê³µì‹ ë¬¸ì„œ ë°©ì‹ ì‚¬ìš©)
    
    Args:
        callback_handler: ì½œë°± í•¸ë“¤ëŸ¬
        model_name: ì‚¬ìš©í•  ëª¨ë¸ (ì˜ˆ: deepseek/deepseek-r1-0528-qwen3-8b:free)
        api_key: OpenRouter API í‚¤
        temperature: ìƒì„± ì˜¨ë„
        max_tokens: ìµœëŒ€ í† í° ìˆ˜
        **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
    
    Returns:
        OpenRouterLLM: OpenRouter LLM ì¸ìŠ¤í„´ìŠ¤
    """
    LOGGER.info(f"ğŸ”§ Creating OpenRouter LLM with model: {model_name}")
    
    # Pydantic ëª¨ë¸ì— ë§ê²Œ í‚¤ì›Œë“œ ì¸ìë¡œ ì „ë‹¬
    return OpenRouterLLM(
        model_name=model_name,
        api_key=api_key,
        base_url=Config.OPENROUTER_BASE_URL,
        temperature=temperature,
        max_tokens=max_tokens or 4000,
        callback_handler=callback_handler
    )