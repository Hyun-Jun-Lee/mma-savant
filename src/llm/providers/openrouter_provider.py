from typing import Callable, Dict, Any

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from config import Config
from common.logging_config import get_logger

LOGGER = get_logger(__name__)




def get_model_specific_params(model_name: str) -> Dict[str, Any]:
    """
    ëª¨ë¸ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„° ë°˜í™˜
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        
    Returns:
        Dict: ëª¨ë¸ì— íŠ¹í™”ëœ íŒŒë¼ë¯¸í„°ë“¤
    """
    model_params = {}
    
    # Llama-4-scout ëª¨ë¸ - extra_bodyë¡œ íŠ¹í™” íŒŒë¼ë¯¸í„° ì „ë‹¬
    if "llama-4-scout" in model_name.lower():
        model_params.update({
            "temperature": 0.7,
            "top_p": 0.9,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "min_rounds": 1,
            "max_rounds": 10
        })
    
    # Llama-3 ê³„ì—´ ëª¨ë¸ë“¤
    elif "llama-3" in model_name.lower():
        model_params.update({
            "top_p": 0.9,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        })
    
    # DeepSeek ëª¨ë¸ë“¤
    elif "deepseek" in model_name.lower():
        model_params.update({
            "top_p": 0.95,
            "temperature": 0.6
        })
    
    # GPT-4 ê³„ì—´ ëª¨ë¸ë“¤
    elif "gpt-4" in model_name.lower():
        model_params.update({
            "top_p": 1.0,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0
        })
    
    # Claude ê³„ì—´ ëª¨ë¸ë“¤
    elif "claude" in model_name.lower():
        model_params.update({
            "top_p": 0.9,
            "temperature": 0.7
        })
    
    # Mixtral ëª¨ë¸ë“¤
    elif "mixtral" in model_name.lower():
        model_params.update({
            "top_p": 0.9,
            "temperature": 0.7
        })
    
    # Gemini ëª¨ë¸ë“¤
    elif "gemini" in model_name.lower():
        model_params.update({
            "top_p": 0.95,
            "temperature": 0.7
        })
    
    return model_params




def get_model_info(model_name: str) -> Dict[str, Any]:
    """
    ëª¨ë¸ ì •ë³´ ë° ê¶Œì¥ ì„¤ì • ë°˜í™˜
    
    Args:
        model_name: ëª¨ë¸ ì´ë¦„
        
    Returns:
        Dict: ëª¨ë¸ ì •ë³´
    """
    if "llama-4-scout" in model_name.lower():
        return {
            "provider": "Meta",
            "type": "chat",
            "context_length": 8192,
            "description": "Metaì˜ ìµœì‹  Llama-4 ìŠ¤ì¹´ìš°íŠ¸ ëª¨ë¸",
            "strengths": ["reasoning", "code", "math"],
            "recommended_temp": 0.7
        }
    
    elif "deepseek" in model_name.lower():
        return {
            "provider": "DeepSeek",
            "type": "chat",
            "context_length": 32768,
            "description": "DeepSeekì˜ ê³ ì„±ëŠ¥ ì¶”ë¡  ëª¨ë¸",
            "strengths": ["reasoning", "code", "analysis"],
            "recommended_temp": 0.6
        }
    
    elif "gpt-4" in model_name.lower():
        return {
            "provider": "OpenAI",
            "type": "chat",
            "context_length": 128000 if "turbo" in model_name.lower() else 8192,
            "description": "OpenAIì˜ ê³ ì„±ëŠ¥ GPT-4 ëª¨ë¸",
            "strengths": ["general", "creative", "analysis"],
            "recommended_temp": 0.7
        }
    
    else:
        return {
            "provider": "Unknown",
            "type": "chat", 
            "context_length": 4096,
            "description": "ì¼ë°˜ ì±„íŒ… ëª¨ë¸",
            "strengths": ["general"],
            "recommended_temp": 0.7
        }


# create_react_prompt_template í•¨ìˆ˜ëŠ” llm.prompts.create_phase1_prompt_template()ë¡œ ì´ë™ë¨


def get_openrouter_llm(
    callback_handler: Callable,
    model_name: str = Config.OPENROUTER_MODEL_NAME,
    api_key: str = Config.OPENROUTER_API_KEY,
    temperature: float = 0.7,
    max_tokens: int = None,
    **kwargs
):
    """
    OpenRouter LLM ìƒì„± - í‘œì¤€ ChatOpenAI ì‚¬ìš© (ReAct ì—ì´ì „íŠ¸ìš©)

    ì£¼ìš” ê¸°ëŠ¥:
    - test_openrouter_2.pyì—ì„œ ê²€ì¦ëœ ì•ˆì •ì ì¸ í‘œì¤€ ChatOpenAI ì‚¬ìš©
    - ëª¨ë¸ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„° ìë™ ì ìš©
    - ìŠ¤íŠ¸ë¦¬ë° ë° ì½œë°± í•¸ë“¤ëŸ¬ ì™„ì „ ì§€ì›
    - OpenRouter ìµœì í™” í—¤ë” ì„¤ì •

    Args:
        callback_handler: ìŠ¤íŠ¸ë¦¬ë°ìš© ì½œë°± í•¸ë“¤ëŸ¬
        model_name: ì‚¬ìš©í•  ëª¨ë¸ (ì˜ˆ: deepseek/deepseek-chat-v3-0324:free)
        api_key: OpenRouter API í‚¤
        temperature: ìƒì„± ì˜¨ë„ (ëª¨ë¸ë³„ ìµœì ê°’ ìë™ ì ìš©)
        max_tokens: ìµœëŒ€ í† í° ìˆ˜

    Returns:
        ChatOpenAI: OpenRouter LLM ì¸ìŠ¤í„´ìŠ¤

    Example:
        >>> from llm.callbacks.openrouter_callback import get_openrouter_callback_handler
        >>> callback = get_openrouter_callback_handler("msg_123", "session_456", "deepseek/deepseek-chat")
        >>> llm = get_openrouter_llm(callback, "deepseek/deepseek-chat")
        >>> # í•­ìƒ ReAct ì—ì´ì „íŠ¸ì™€ í•¨ê»˜ ì‚¬ìš©
    """
    LOGGER.info(f"ğŸ”§ Creating OpenRouter LLM with model: {model_name}")

    # ëª¨ë¸ë³„ íŠ¹í™” íŒŒë¼ë¯¸í„° ì ìš©
    model_params = get_model_specific_params(model_name)
    effective_temperature = model_params.get("temperature", temperature)
    effective_max_tokens = model_params.get("max_tokens", max_tokens or 4000)

    LOGGER.info(f"ğŸ“± Using ChatOpenAI for ReAct agent with temp={effective_temperature}, max_tokens={effective_max_tokens}")

    return ChatOpenAI(
        model=model_name,
        api_key=api_key,
        base_url=Config.OPENROUTER_BASE_URL,
        temperature=effective_temperature,
        max_tokens=effective_max_tokens,
        default_headers={
            "HTTP-Referer": "https://mma-savant.com",
            "X-Title": "MMA Savant"
        },
        callbacks=[callback_handler] if callback_handler else [],
        streaming=True
    )