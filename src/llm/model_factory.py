"""
LLM Î™®Îç∏ Î∞è ÏΩúÎ∞± Ìï∏Îì§Îü¨ ÏÉùÏÑ± Ìå©ÌÜ†Î¶¨
Îã§ÏñëÌïú ÌîÑÎ°úÎ∞îÏù¥ÎçîÎ•º ÏßÄÏõêÌïòÎ©∞ ÌôòÍ≤Ω Î≥ÄÏàòÎ•º ÌÜµÌïú ÎèôÏ†Å Î™®Îç∏ ÏÑ†ÌÉù Ï†úÍ≥µ
"""
from typing import List, Tuple, Dict, Any, Optional
from enum import Enum

from config import Config
from common.logging_config import get_logger
from common.enums import LLMProvider
from llm.providers import get_anthropic_llm, get_huggingface_llm, get_chat_model_llm
from llm.providers.openrouter_provider import get_openrouter_llm
from llm.callbacks import get_anthropic_callback_handler, get_huggingface_callback_handler
from llm.callbacks.openrouter_callback import get_openrouter_callback_handler

LOGGER = get_logger(__name__)





def create_llm_with_callbacks(
    message_id: str,
    session_id: str,
    provider: Optional[str] = None,
    **model_kwargs
) -> Tuple[Any, Any]:
    """
    ÌîÑÎ°úÎ∞îÏù¥ÎçîÏóê Îî∞Î•∏ LLMÍ≥º ÏΩúÎ∞± Ìï∏Îì§Îü¨ ÏÉùÏÑ±
    
    Args:
        message_id: Î©îÏãúÏßÄ ID
        session_id: ÏÑ∏ÏÖò ID  
        provider: LLM ÌîÑÎ°úÎ∞îÏù¥Îçî (NoneÏù¥Î©¥ Config.LLM_PROVIDER ÏÇ¨Ïö©)
        **model_kwargs: Î™®Îç∏Î≥Ñ Ï∂îÍ∞Ä ÌååÎùºÎØ∏ÌÑ∞
        
    Returns:
        Tuple[LLM, CallbackHandler]: ÏÉùÏÑ±Îêú LLMÍ≥º ÏΩúÎ∞± Ìï∏Îì§Îü¨
        
    Raises:
        ValueError: ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌîÑÎ°úÎ∞îÏù¥ÎçîÏù¥Í±∞ÎÇò ÏÑ§Ï†ïÏù¥ ÏûòÎ™ªÎêú Í≤ΩÏö∞
        ImportError: ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏùÄ Í≤ΩÏö∞
    """
    # ÌîÑÎ°úÎ∞îÏù¥Îçî Í≤∞Ï†ï
    selected_provider = provider or Config.LLM_PROVIDER
    
    LOGGER.info(f"üè≠ Creating LLM with provider: {selected_provider}")
    
    try:
        if selected_provider == LLMProvider.ANTHROPIC.value:
            return get_anthropic_model_and_callback(message_id, session_id, **model_kwargs)
            
        elif selected_provider == LLMProvider.HUGGINGFACE.value:
            return get_huggingface_model_and_callback(message_id, session_id, **model_kwargs)
            
        elif selected_provider == LLMProvider.OPENROUTER.value:
            return get_openrouter_model_and_callback(message_id, session_id, **model_kwargs)
            
        elif selected_provider == LLMProvider.OPENAI.value:
            return get_openai_model_and_callback(message_id, session_id, **model_kwargs)
            
        else:
            available = [p.value for p in LLMProvider]
            raise ValueError(f"Unsupported provider: {selected_provider}. Available: {available}")
            
    except ImportError as e:
        LOGGER.error(f"‚ùå Missing required library for provider {selected_provider}: {e}")
        raise ImportError(f"Provider {selected_provider} requires additional libraries. {e}")
    except Exception as e:
        LOGGER.error(f"‚ùå Error creating LLM for provider {selected_provider}: {e}")
        raise


def get_anthropic_model_and_callback(
    message_id: str, 
    session_id: str,
    **kwargs
) -> Tuple[Any, Any]:
    """
    Anthropic Î™®Îç∏Í≥º ÏΩúÎ∞± ÏÉùÏÑ±
    
    Args:
        message_id: Î©îÏãúÏßÄ ID
        session_id: ÏÑ∏ÏÖò ID
        **kwargs: Ï∂îÍ∞Ä Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ (Ïò®ÎèÑ, ÏµúÎåÄ ÌÜ†ÌÅ∞ Îì±)
        
    Returns:
        Tuple[AnthropicLLM, AnthropicCallbackHandler]
    """
    # ÏÑ§Ï†ï Í≤ÄÏ¶ù
    if not validate_provider_config(LLMProvider.ANTHROPIC.value):
        raise ValueError("Anthropic configuration is invalid. Check ANTHROPIC_API_KEY.")
    
    try:
        # ÏΩúÎ∞± Ìï∏Îì§Îü¨ ÏÉùÏÑ±
        callback_handler = get_anthropic_callback_handler(message_id, session_id)
        
        # Î™®Îç∏Î≥Ñ ÌååÎùºÎØ∏ÌÑ∞ Ï†ÅÏö©
        model_params = {
            "callback_handler": callback_handler,
            **kwargs
        }
        
        # LLM ÏÉùÏÑ±
        llm = get_anthropic_llm(**model_params)
        
        LOGGER.info(f"‚úÖ Anthropic LLM created successfully")
        return llm, callback_handler
        
    except ImportError as e:
        LOGGER.error(f"‚ùå Failed to import Anthropic modules: {e}")
        raise ImportError("Anthropic provider requires 'anthropic' package")
    except Exception as e:
        LOGGER.error(f"‚ùå Error creating Anthropic model: {e}")
        raise


def get_huggingface_model_and_callback(
    message_id: str,
    session_id: str, 
    model_name: Optional[str] = None,
    **kwargs
) -> Tuple[Any, Any]:
    """
    HuggingFace Î™®Îç∏Í≥º ÏΩúÎ∞± ÏÉùÏÑ±
    
    Args:
        message_id: Î©îÏãúÏßÄ ID
        session_id: ÏÑ∏ÏÖò ID
        model_name: Î™®Îç∏ Ïù¥Î¶Ñ (NoneÏù¥Î©¥ Config.HUGGINGFACE_MODEL_NAME ÏÇ¨Ïö©)
        **kwargs: Ï∂îÍ∞Ä Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞
        
    Returns:
        Tuple[HuggingFaceEndpoint, HuggingFaceCallbackHandler]
    """
    # Î™®Îç∏ Ïù¥Î¶Ñ Í≤∞Ï†ï
    final_model_name = model_name or Config.HUGGINGFACE_MODEL_NAME
    
    try:
        
        # ÏΩúÎ∞± Ìï∏Îì§Îü¨ ÏÉùÏÑ±
        callback_handler = get_huggingface_callback_handler(
            message_id, 
            session_id, 
            model_name=final_model_name
        )

        # ÏÑ§Ï†ïÏóêÏÑú Í∏∞Î≥∏ ÌååÎùºÎØ∏ÌÑ∞ Í∞ÄÏ†∏Ïò§Í∏∞ (API Î∞©Ïãù)
        model_params = {
            "callback_handler": callback_handler,
            "model_name": final_model_name,
            "temperature": kwargs.get("temperature", Config.HUGGINGFACE_TEMPERATURE),
            "max_tokens": kwargs.get("max_tokens", Config.HUGGINGFACE_MAX_TOKENS),
            "huggingface_api_token": kwargs.get("huggingface_api_token", Config.HUGGINGFACE_API_TOKEN),
            **{k: v for k, v in kwargs.items() if k not in ["temperature", "max_tokens", "huggingface_api_token"]}
        }
        
        # LLM ÏÉùÏÑ± (Chat Î™®Îç∏ ÏÇ¨Ïö©)
        if kwargs.get("use_chat_model", True):
            # Ï±ÑÌåÖ ÏµúÏ†ÅÌôî Î™®Îç∏ ÏÇ¨Ïö©
            model_type = kwargs.get("model_type", "dialogpt")
            model_size = kwargs.get("model_size", "medium")
            llm = get_chat_model_llm(
                callback_handler=callback_handler,
                model_type=model_type,
                size=model_size,
                huggingface_api_token=model_params["huggingface_api_token"],
                temperature=model_params["temperature"],
                max_tokens=model_params["max_tokens"]
            )
        else:
            # ÏßÅÏ†ë Î™®Îç∏ ÏßÄÏ†ï
            llm = get_huggingface_llm(**model_params)
        
        LOGGER.info(f"‚úÖ HuggingFace LLM created: {final_model_name}")
        return llm, callback_handler
        
    except ImportError as e:
        LOGGER.error(f"‚ùå Failed to import HuggingFace modules: {e}")
        raise ImportError("HuggingFace provider requires 'langchain-huggingface' package")
    except Exception as e:
        LOGGER.error(f"‚ùå Error creating HuggingFace model: {e}")
        raise


def get_openrouter_model_and_callback(
    message_id: str,
    session_id: str,
    model_name: Optional[str] = None,
    **kwargs
) -> Tuple[Any, Any]:
    """
    OpenRouter Î™®Îç∏Í≥º ÏΩúÎ∞± ÏÉùÏÑ±
    
    Args:
        message_id: Î©îÏãúÏßÄ ID
        session_id: ÏÑ∏ÏÖò ID
        model_name: Î™®Îç∏ Ïù¥Î¶Ñ (NoneÏù¥Î©¥ Config.OPENROUTER_MODEL_NAME ÏÇ¨Ïö©)
        **kwargs: Ï∂îÍ∞Ä Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞
        
    Returns:
        Tuple[ChatOpenAI, OpenRouterCallbackHandler]
    """
    # ÏÑ§Ï†ï Í≤ÄÏ¶ù
    if not validate_provider_config(LLMProvider.OPENROUTER.value):
        raise ValueError("OpenRouter configuration is invalid. Check OPENROUTER_API_KEY.")
    
    # Î™®Îç∏ Ïù¥Î¶Ñ Í≤∞Ï†ï
    final_model_name = model_name or Config.OPENROUTER_MODEL_NAME
    
    try:
        # ÏΩúÎ∞± Ìï∏Îì§Îü¨ ÏÉùÏÑ±
        callback_handler = get_openrouter_callback_handler(
            message_id=message_id,
            session_id=session_id,
            model_name=final_model_name
        )
        
        # Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï
        model_params = {
            "callback_handler": callback_handler,
            "model_name": final_model_name,
            "api_key": kwargs.get("api_key", Config.OPENROUTER_API_KEY),
            "temperature": kwargs.get("temperature", 0.7),
            "max_tokens": kwargs.get("max_tokens", 4000),
            **{k: v for k, v in kwargs.items() if k not in ["api_key", "temperature", "max_tokens"]}
        }
        
        # LLM ÏÉùÏÑ±
        llm = get_openrouter_llm(**model_params)
        
        LOGGER.info(f"‚úÖ OpenRouter LLM created: {final_model_name}")
        return llm, callback_handler
        
    except ImportError as e:
        LOGGER.error(f"‚ùå Failed to import OpenRouter modules: {e}")
        raise ImportError("OpenRouter provider requires 'langchain-openai' package")
    except Exception as e:
        LOGGER.error(f"‚ùå Error creating OpenRouter model: {e}")
        raise


def get_openai_model_and_callback(
    message_id: str, 
    session_id: str,
    **kwargs
) -> Tuple[Any, Any]:
    """
    OpenAI Î™®Îç∏Í≥º ÏΩúÎ∞± ÏÉùÏÑ± (Ìñ•ÌõÑ ÌôïÏû•Ïö©)
    
    Args:
        message_id: Î©îÏãúÏßÄ ID
        session_id: ÏÑ∏ÏÖò ID
        **kwargs: Ï∂îÍ∞Ä Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞
        
    Returns:
        Tuple[OpenAILLM, OpenAICallbackHandler]
        
    Raises:
        NotImplementedError: ÏïÑÏßÅ Íµ¨ÌòÑÎêòÏßÄ ÏïäÏùå
    """
    # ÏÑ§Ï†ï Í≤ÄÏ¶ù
    if not validate_provider_config(LLMProvider.OPENAI.value):
        raise ValueError("OpenAI configuration is invalid. Check OPENAI_API_KEY.")
    
    # TODO: OpenAI Íµ¨ÌòÑ
    raise NotImplementedError("OpenAI provider will be implemented in future versions")


def get_available_providers() -> List[str]:
    """
    ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÌîÑÎ°úÎ∞îÏù¥Îçî Î™©Î°ù Î∞òÌôò
    
    ÏÑ§Ï†ïÏù¥ Ïò¨Î∞îÎ•¥Í≤å ÎêòÏñ¥ÏûàÍ≥† ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêú ÌîÑÎ°úÎ∞îÏù¥ÎçîÎßå Î∞òÌôò
    
    Returns:
        List[str]: ÏÇ¨Ïö© Í∞ÄÎä•Ìïú ÌîÑÎ°úÎ∞îÏù¥Îçî Î™©Î°ù
    """
    available = []
    
    for provider in LLMProvider:
        try:
            if validate_provider_config(provider.value):
                # Ïã§Ï†ú import ÌÖåÏä§Ìä∏
                if provider == LLMProvider.ANTHROPIC:
                    available.append(provider.value)
                    
                elif provider == LLMProvider.HUGGINGFACE:
                    available.append(provider.value)
                    
                elif provider == LLMProvider.OPENROUTER:
                    available.append(provider.value)
                    
                elif provider == LLMProvider.OPENAI:
                    # TODO: OpenAI import ÌÖåÏä§Ìä∏
                    pass
                    
        except ImportError:
            LOGGER.debug(f"Provider {provider.value} not available due to missing dependencies")
        except Exception as e:
            LOGGER.debug(f"Provider {provider.value} not available: {e}")
    
    LOGGER.info(f"üîç Available providers: {available}")
    return available


def validate_provider_config(provider: str) -> bool:
    """
    ÌîÑÎ°úÎ∞îÏù¥Îçî ÏÑ§Ï†ï Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨
    
    Args:
        provider: ÌîÑÎ°úÎ∞îÏù¥Îçî Ïù¥Î¶Ñ
        
    Returns:
        bool: ÏÑ§Ï†ïÏù¥ Ïú†Ìö®ÌïúÏßÄ Ïó¨Î∂Ä
    """
    if provider == LLMProvider.ANTHROPIC.value:
        valid = bool(Config.ANTHROPIC_API_KEY and Config.ANTHROPIC_API_KEY.strip())
        if not valid:
            LOGGER.warning("‚ö†Ô∏è Anthropic API key not configured")
        return valid
        
    elif provider == LLMProvider.HUGGINGFACE.value:
        # Î™®Îç∏ Ïù¥Î¶Ñ ÏÑ§Ï†ï ÌôïÏù∏
        model_configured = bool(Config.HUGGINGFACE_MODEL_NAME and Config.HUGGINGFACE_MODEL_NAME.strip())
        if not model_configured:
            LOGGER.warning("‚ö†Ô∏è HuggingFace model name not configured")
            return False
        
        return True
        
    elif provider == LLMProvider.OPENROUTER.value:
        valid = bool(Config.OPENROUTER_API_KEY and Config.OPENROUTER_API_KEY.strip())
        if not valid:
            LOGGER.warning("‚ö†Ô∏è OpenRouter API key not configured")
        return valid
        
    elif provider == LLMProvider.OPENAI.value:
        valid = bool(Config.OPENAI_API_KEY and Config.OPENAI_API_KEY.strip())
        if not valid:
            LOGGER.warning("‚ö†Ô∏è OpenAI API key not configured")
        return valid
        
    else:
        LOGGER.warning(f"‚ö†Ô∏è Unknown provider: {provider}")
        return False