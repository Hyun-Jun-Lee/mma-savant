"""
LLM ëª¨ë¸ ë° ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„± íŒ©í† ë¦¬
ë‹¤ì–‘í•œ í”„ë¡œë°”ì´ë”ë¥¼ ì§€ì›í•˜ë©° í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•œ ë™ì  ëª¨ë¸ ì„ íƒ ì œê³µ
"""
from typing import List, Tuple, Dict, Any, Optional
from enum import Enum

from config import Config
from common.logging_config import get_logger
from llm.providers import get_anthropic_llm, get_huggingface_llm, get_chat_model_llm
from llm.callbacks import get_anthropic_callback_handler, get_huggingface_callback_handler

LOGGER = get_logger(__name__)


class LLMProvider(Enum):
    """ì§€ì›ë˜ëŠ” LLM í”„ë¡œë°”ì´ë”"""
    ANTHROPIC = "anthropic"
    HUGGINGFACE = "huggingface"
    OPENAI = "openai"  # í–¥í›„ í™•ì¥ìš©


def create_llm_with_callbacks(
    message_id: str,
    session_id: str,
    provider: Optional[str] = None,
    **model_kwargs
) -> Tuple[Any, Any]:
    """
    í”„ë¡œë°”ì´ë”ì— ë”°ë¥¸ LLMê³¼ ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
    
    Args:
        message_id: ë©”ì‹œì§€ ID
        session_id: ì„¸ì…˜ ID  
        provider: LLM í”„ë¡œë°”ì´ë” (Noneì´ë©´ Config.LLM_PROVIDER ì‚¬ìš©)
        **model_kwargs: ëª¨ë¸ë³„ ì¶”ê°€ íŒŒë¼ë¯¸í„°
        
    Returns:
        Tuple[LLM, CallbackHandler]: ìƒì„±ëœ LLMê³¼ ì½œë°± í•¸ë“¤ëŸ¬
        
    Raises:
        ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œë°”ì´ë”ì´ê±°ë‚˜ ì„¤ì •ì´ ì˜ëª»ëœ ê²½ìš°
        ImportError: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
    """
    # í”„ë¡œë°”ì´ë” ê²°ì •
    selected_provider = provider or Config.LLM_PROVIDER
    
    LOGGER.info(f"ğŸ­ Creating LLM with provider: {selected_provider}")
    
    try:
        if selected_provider == LLMProvider.ANTHROPIC.value:
            return get_anthropic_model_and_callback(message_id, session_id, **model_kwargs)
            
        elif selected_provider == LLMProvider.HUGGINGFACE.value:
            return get_huggingface_model_and_callback(message_id, session_id, **model_kwargs)
            
        elif selected_provider == LLMProvider.OPENAI.value:
            return get_openai_model_and_callback(message_id, session_id, **model_kwargs)
            
        else:
            available = [p.value for p in LLMProvider]
            raise ValueError(f"Unsupported provider: {selected_provider}. Available: {available}")
            
    except ImportError as e:
        LOGGER.error(f"âŒ Missing required library for provider {selected_provider}: {e}")
        raise ImportError(f"Provider {selected_provider} requires additional libraries. {e}")
    except Exception as e:
        LOGGER.error(f"âŒ Error creating LLM for provider {selected_provider}: {e}")
        raise


def get_anthropic_model_and_callback(
    message_id: str, 
    session_id: str,
    **kwargs
) -> Tuple[Any, Any]:
    """
    Anthropic ëª¨ë¸ê³¼ ì½œë°± ìƒì„±
    
    Args:
        message_id: ë©”ì‹œì§€ ID
        session_id: ì„¸ì…˜ ID
        **kwargs: ì¶”ê°€ ëª¨ë¸ íŒŒë¼ë¯¸í„° (ì˜¨ë„, ìµœëŒ€ í† í° ë“±)
        
    Returns:
        Tuple[AnthropicLLM, AnthropicCallbackHandler]
    """
    # ì„¤ì • ê²€ì¦
    if not validate_provider_config(LLMProvider.ANTHROPIC.value):
        raise ValueError("Anthropic configuration is invalid. Check ANTHROPIC_API_KEY.")
    
    try:
        # ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
        callback_handler = get_anthropic_callback_handler(message_id, session_id)
        
        # ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„° ì ìš©
        model_params = {
            "callback_handler": callback_handler,
            **kwargs
        }
        
        # LLM ìƒì„±
        llm = get_anthropic_llm(**model_params)
        
        LOGGER.info(f"âœ… Anthropic LLM created successfully")
        return llm, callback_handler
        
    except ImportError as e:
        LOGGER.error(f"âŒ Failed to import Anthropic modules: {e}")
        raise ImportError("Anthropic provider requires 'anthropic' package")
    except Exception as e:
        LOGGER.error(f"âŒ Error creating Anthropic model: {e}")
        raise


def get_huggingface_model_and_callback(
    message_id: str,
    session_id: str, 
    model_name: Optional[str] = None,
    **kwargs
) -> Tuple[Any, Any]:
    """
    HuggingFace ëª¨ë¸ê³¼ ì½œë°± ìƒì„±
    
    Args:
        message_id: ë©”ì‹œì§€ ID
        session_id: ì„¸ì…˜ ID
        model_name: ëª¨ë¸ ì´ë¦„ (Noneì´ë©´ Config.HUGGINGFACE_MODEL_NAME ì‚¬ìš©)
        **kwargs: ì¶”ê°€ ëª¨ë¸ íŒŒë¼ë¯¸í„°
        
    Returns:
        Tuple[HuggingFaceEndpoint, HuggingFaceCallbackHandler]
    """
    # ëª¨ë¸ ì´ë¦„ ê²°ì •
    final_model_name = model_name or Config.HUGGINGFACE_MODEL_NAME
    
    try:
        
        # ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
        callback_handler = get_huggingface_callback_handler(
            message_id, 
            session_id, 
            model_name=final_model_name
        )
        
        # API í† í° ê²€ì¦
        if not validate_huggingface_api_config():
            LOGGER.warning("âš ï¸ HuggingFace API token not configured, using public models only")
        
        # ì„¤ì •ì—ì„œ ê¸°ë³¸ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸° (API ë°©ì‹)
        model_params = {
            "callback_handler": callback_handler,
            "model_name": final_model_name,
            "temperature": kwargs.get("temperature", Config.HUGGINGFACE_TEMPERATURE),
            "max_tokens": kwargs.get("max_tokens", Config.HUGGINGFACE_MAX_TOKENS),
            "huggingface_api_token": kwargs.get("huggingface_api_token", Config.HUGGINGFACE_API_TOKEN),
            **{k: v for k, v in kwargs.items() if k not in ["temperature", "max_tokens", "huggingface_api_token"]}
        }
        
        # LLM ìƒì„± (Chat ëª¨ë¸ ì‚¬ìš©)
        if kwargs.get("use_chat_model", True):
            # ì±„íŒ… ìµœì í™” ëª¨ë¸ ì‚¬ìš©
            model_type = kwargs.get("model_type", "dialogpt")
            model_size = kwargs.get("model_size", "medium")
            llm = get_chat_model_llm(
                callback_handler=callback_handler,
                model_type=model_type,
                size=model_size,
                huggingface_api_token=model_params["huggingface_api_token"],
                temperature=model_params["temperature"],
                max_tokens=model_params["max_tokens"]
            )
        else:
            # ì§ì ‘ ëª¨ë¸ ì§€ì •
            llm = get_huggingface_llm(**model_params)
        
        LOGGER.info(f"âœ… HuggingFace LLM created: {final_model_name}")
        return llm, callback_handler
        
    except ImportError as e:
        LOGGER.error(f"âŒ Failed to import HuggingFace modules: {e}")
        raise ImportError("HuggingFace provider requires 'langchain-huggingface' package")
    except Exception as e:
        LOGGER.error(f"âŒ Error creating HuggingFace model: {e}")
        raise


def get_openai_model_and_callback(
    message_id: str, 
    session_id: str,
    **kwargs
) -> Tuple[Any, Any]:
    """
    OpenAI ëª¨ë¸ê³¼ ì½œë°± ìƒì„± (í–¥í›„ í™•ì¥ìš©)
    
    Args:
        message_id: ë©”ì‹œì§€ ID
        session_id: ì„¸ì…˜ ID
        **kwargs: ì¶”ê°€ ëª¨ë¸ íŒŒë¼ë¯¸í„°
        
    Returns:
        Tuple[OpenAILLM, OpenAICallbackHandler]
        
    Raises:
        NotImplementedError: ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ
    """
    # ì„¤ì • ê²€ì¦
    if not validate_provider_config(LLMProvider.OPENAI.value):
        raise ValueError("OpenAI configuration is invalid. Check OPENAI_API_KEY.")
    
    # TODO: OpenAI êµ¬í˜„
    raise NotImplementedError("OpenAI provider will be implemented in future versions")


def get_available_providers() -> List[str]:
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë” ëª©ë¡ ë°˜í™˜
    
    ì„¤ì •ì´ ì˜¬ë°”ë¥´ê²Œ ë˜ì–´ìˆê³  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ëœ í”„ë¡œë°”ì´ë”ë§Œ ë°˜í™˜
    
    Returns:
        List[str]: ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡œë°”ì´ë” ëª©ë¡
    """
    available = []
    
    for provider in LLMProvider:
        try:
            if validate_provider_config(provider.value):
                # ì‹¤ì œ import í…ŒìŠ¤íŠ¸
                if provider == LLMProvider.ANTHROPIC:
                    available.append(provider.value)
                    
                elif provider == LLMProvider.HUGGINGFACE:
                    available.append(provider.value)
                    
                elif provider == LLMProvider.OPENAI:
                    # TODO: OpenAI import í…ŒìŠ¤íŠ¸
                    pass
                    
        except ImportError:
            LOGGER.debug(f"Provider {provider.value} not available due to missing dependencies")
        except Exception as e:
            LOGGER.debug(f"Provider {provider.value} not available: {e}")
    
    LOGGER.info(f"ğŸ” Available providers: {available}")
    return available


def validate_provider_config(provider: str) -> bool:
    """
    í”„ë¡œë°”ì´ë” ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬
    
    Args:
        provider: í”„ë¡œë°”ì´ë” ì´ë¦„
        
    Returns:
        bool: ì„¤ì •ì´ ìœ íš¨í•œì§€ ì—¬ë¶€
    """
    if provider == LLMProvider.ANTHROPIC.value:
        valid = bool(Config.ANTHROPIC_API_KEY and Config.ANTHROPIC_API_KEY.strip())
        if not valid:
            LOGGER.warning("âš ï¸ Anthropic API key not configured")
        return valid
        
    elif provider == LLMProvider.HUGGINGFACE.value:
        # ëª¨ë¸ ì´ë¦„ ì„¤ì • í™•ì¸
        model_configured = bool(Config.HUGGINGFACE_MODEL_NAME and Config.HUGGINGFACE_MODEL_NAME.strip())
        if not model_configured:
            LOGGER.warning("âš ï¸ HuggingFace model name not configured")
            return False
        
        # API í† í°ì€ ì„ íƒì‚¬í•­ (public ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥)
        return True
        
    elif provider == LLMProvider.OPENAI.value:
        valid = bool(Config.OPENAI_API_KEY and Config.OPENAI_API_KEY.strip())
        if not valid:
            LOGGER.warning("âš ï¸ OpenAI API key not configured")
        return valid
        
    else:
        LOGGER.warning(f"âš ï¸ Unknown provider: {provider}")
        return False


def get_provider_info() -> Dict[str, Dict[str, Any]]:
    """
    ëª¨ë“  í”„ë¡œë°”ì´ë”ì˜ ìƒì„¸ ì •ë³´ ë°˜í™˜
    
    Returns:
        Dict: í”„ë¡œë°”ì´ë”ë³„ ì •ë³´ (ì„¤ì • ìƒíƒœ, ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ë“±)
    """
    info = {}
    
    for provider in LLMProvider:
        provider_name = provider.value
        is_available = provider_name in get_available_providers()
        is_configured = validate_provider_config(provider_name)
        
        provider_info = {
            "available": is_available,
            "configured": is_configured,
            "description": _get_provider_description(provider_name)
        }
        
        # í”„ë¡œë°”ì´ë”ë³„ ì¶”ê°€ ì •ë³´
        if provider_name == LLMProvider.ANTHROPIC.value:
            provider_info.update({
                "model_name": getattr(Config, 'ANTHROPIC_MODEL_NAME', 'Not configured'),
                "api_key_configured": bool(getattr(Config, 'ANTHROPIC_API_KEY', None))
            })
        elif provider_name == LLMProvider.HUGGINGFACE.value:
            provider_info.update({
                "model_name": Config.HUGGINGFACE_MODEL_NAME,
                "api_token_configured": bool(Config.HUGGINGFACE_API_TOKEN),
                "max_tokens": Config.HUGGINGFACE_MAX_TOKENS,
                "temperature": Config.HUGGINGFACE_TEMPERATURE,
                "api_mode": "inference_api"
            })
        elif provider_name == LLMProvider.OPENAI.value:
            provider_info.update({
                "model_name": getattr(Config, 'OPENAI_MODEL_NAME', 'Not configured'),
                "api_key_configured": bool(getattr(Config, 'OPENAI_API_KEY', None))
            })
            
        info[provider_name] = provider_info
    
    return info


def validate_huggingface_api_config() -> bool:
    """
    HuggingFace API ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬
    
    Returns:
        bool: API í† í°ì´ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ ì—¬ë¶€
    """
    return bool(Config.HUGGINGFACE_API_TOKEN and Config.HUGGINGFACE_API_TOKEN.strip())


def _get_provider_description(provider: str) -> str:
    """í”„ë¡œë°”ì´ë” ì„¤ëª… ë°˜í™˜"""
    descriptions = {
        LLMProvider.ANTHROPIC.value: "Claude models from Anthropic - high quality reasoning and analysis",
        LLMProvider.HUGGINGFACE.value: "Open source models from HuggingFace Inference API - cloud-based execution",
        LLMProvider.OPENAI.value: "GPT models from OpenAI - powerful general purpose AI (coming soon)"
    }
    return descriptions.get(provider, "Unknown provider")


if __name__ == "__main__":
    """í…ŒìŠ¤íŠ¸ ë° ë””ë²„ê¹…ìš©"""
    print("ğŸ­ Model Factory Test")
    print("=" * 50)
    
    print("\nğŸ“‹ Available Providers:")
    available = get_available_providers()
    for provider in available:
        print(f"  âœ… {provider}")
    
    # print(f"\nğŸ¯ Recommended Provider: {get_recommended_provider()}")  # í•¨ìˆ˜ê°€ ì—†ì–´ì„œ ì£¼ì„ì²˜ë¦¬
    
    print(f"\nğŸ”§ Current Configuration:")
    print(f"  LLM_PROVIDER: {Config.LLM_PROVIDER}")
    print(f"  ANTHROPIC_API_KEY configured: {bool(getattr(Config, 'ANTHROPIC_API_KEY', None))}")
    print(f"  HUGGINGFACE_MODEL_NAME: {Config.HUGGINGFACE_MODEL_NAME}")
    
    print("\nğŸ“Š Provider Details:")
    info = get_provider_info()
    for provider, details in info.items():
        print(f"  {provider}:")
        for key, value in details.items():
            print(f"    {key}: {value}")